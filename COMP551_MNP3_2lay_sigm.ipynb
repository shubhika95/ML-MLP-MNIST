{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6331ZSsQGY3"
   },
   "source": [
    "# COMP 551 - Mini-project 3\n",
    "Group 63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ifel1K0gBWt_"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "%matplotlib inline                                 \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.debugger import set_trace  \n",
    "import scipy.sparse as sparse\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b08Mmbs8lr81"
   },
   "source": [
    "## Task 1. Data pre-processing\n",
    "\n",
    "- Load the raw data from Keras.\n",
    "- Vectorize 28*28 pictures to 1D vector.\n",
    "- Normalize the intensity of the pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZyGXlaKojgz"
   },
   "source": [
    "Load the MNIST dataset distributed with Keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N01fKjwJDKAz",
    "outputId": "948b6940-b206-4271-b6c3-340b5edbe6e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28) (60000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rX8l1wVDOiN"
   },
   "source": [
    "Vectorize the 28*28 pictures to a 784 vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y4gvmJN6DPxI",
    "outputId": "0bb2322b-e30f-4b5f-93c8-d2a6b0c015fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.reshape(x_train, (-1, 784)).astype('float32')\n",
    "x_test = np.reshape(x_test, (-1, 784)).astype('float32')\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQqnKrdkEcw6"
   },
   "source": [
    "The intensity ranges from 0 to 255. We divide all intensities by the maximum (255) to obtain a [0-1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P-s4iFV3Ez3H",
    "outputId": "239d5baf-0a1d-44ec-d826-e51009a4796f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intensity before normalization: 0.0 255.0\n",
      "Intensity after normalization: 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "print('Intensity before normalization:', np.amin(x_train), np.amax(x_train))\n",
    "x_train, x_test = x_train/255.0, x_test/255.0\n",
    "print('Intensity after normalization:', np.amin(x_train), np.amax(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUnquDPg1D-k"
   },
   "source": [
    "We transform the (N,) vector of labels using one-hot encoding into a (N,C) matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9fXA7QipDn4Z"
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9j0ZFP-81zIY",
    "outputId": "916f7b5d-af3e-4ba2-9b4d-818624f92ed6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10) (10000, 10)\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape, y_test.shape)\n",
    "print(y_train[0:3,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpKU5kZOJkEe"
   },
   "source": [
    "## Task 2. Multilayer perceptron implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6FNYQoRs0ue"
   },
   "source": [
    "### 2.1 Build the network\n",
    "Our task is a multiclass classification.The cost function will be the multi-class cross-entropy loss. We will use the following architecture:\n",
    "- output layer = softmax activation\n",
    "- 2 hidden layers: 128 units, logistic activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUW6HFnH43cv"
   },
   "source": [
    "First, we implement the activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "L1skqPZ_s0uq"
   },
   "outputs": [],
   "source": [
    "# logistic/sigmoid\n",
    "logistic = lambda z: 1./ (1 + np.exp(-z))\n",
    "\n",
    "# softmax\n",
    "eps=1e-8\n",
    "def softmax(z):\n",
    "    logits = z - np.max(z) # for numerical stability\n",
    "    sum_logits = np.sum(np.exp(logits), axis=1) +eps\n",
    "    softmax = np.exp(logits)/sum_logits[:,None] \n",
    "    return softmax\n",
    "\n",
    "# derivatives of sigmoid\n",
    "def deriv_logistic(q):\n",
    "  return logistic(q) * (1 - logistic(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxFKbS58_69S"
   },
   "source": [
    "Next, we build the MLP class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "oKIoaax3f6f0"
   },
   "outputs": [],
   "source": [
    "class MLP2layer_logistic:\n",
    "    \n",
    "    def __init__(self, M = 128, cost=False):\n",
    "        self.M = M\n",
    "        self.cost = cost\n",
    "            \n",
    "    def fit(self, x_train, y_train, x_test, y_test, optimizer):\n",
    "        N = x_train.shape[0]\n",
    "        C = y_train.shape[1] # number of classes\n",
    "        D = x_train.shape[1]\n",
    "        def gradient(x, y, params):\n",
    "            v, w, u = params\n",
    "            # forward pass\n",
    "            n = x.shape[0]\n",
    "            # add bias to the input layer\n",
    "            x = np.column_stack([x,np.ones(n)*0.1])\n",
    "            # add bias to the hidden layers\n",
    "            b = np.ones((n,1))*0.1\n",
    "            q1 = np.dot(x, v) \n",
    "            z1 = logistic(np.hstack((q1,b))) \n",
    "            q2 = np.dot(z1,w) \n",
    "            z2 = logistic(np.hstack((q2,b)))\n",
    "            yh = softmax(np.dot(z2, u))#N x C\n",
    "            # backward pass \n",
    "            dy = yh - y #N x C\n",
    "            du = np.dot(z2.T,dy)/N  \n",
    "            dz2 = np.dot(dy,u.T)\n",
    "            dz2 = np.delete(dz2, -1, axis=1)\n",
    "            dq2 = deriv_logistic(q2)\n",
    "            dw = np.dot(z1.T, dz2 * dq2)/N #M x C\n",
    "            dz1 = np.dot(dz2, w.T) #N x M\n",
    "            dz1 = np.delete(dz1,-1,axis=1)\n",
    "            dq1 = deriv_logistic(q1)\n",
    "            dv = np.dot(x.T, dz1 * dq1)/N #D x M\n",
    "            dparams = [dv, dw, du]\n",
    "            return dparams\n",
    "        \n",
    "        # initialize the parameters with values in the standard normal distribution and scaled to be low\n",
    "        u = np.random.randn(self.M+1,C) * 0.1 #M x C\n",
    "        w = np.random.randn(self.M+1,self.M) * .01 #M x M\n",
    "        v = np.random.randn(D+1,self.M) * .01 #D x M\n",
    "        \n",
    "        params0 = [v,w,u]\n",
    "\n",
    "        # run the mini-batch gradient descent to update the parameters\n",
    "        if self.cost == True:\n",
    "            self.params, self.train_loss, self.test_loss = optimizer.run(gradient, x_train, y_train, x_test, y_test, params0)\n",
    "        else:\n",
    "            self.params = optimizer.run(gradient, x_train, y_train, x_test, y_test, params0)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        v, w, u = self.params\n",
    "        # add bias to the input layer\n",
    "        Nt = x.shape[0]\n",
    "        x = np.column_stack([x,np.ones(Nt)*0.1])\n",
    "        # add bias to the hidden layers\n",
    "        b1 = np.ones((Nt,1))*0.1\n",
    "        # forward pass only using updated parameters\n",
    "        q1 = np.dot(x,v)\n",
    "        z1 = logistic(np.hstack((q1,b1)))\n",
    "        q2 = np.dot(z1,w)\n",
    "        z2 = logistic(np.hstack((q2,b1)))\n",
    "        yh = softmax(np.dot(z2, u))#N x C\n",
    "        return yh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65sh_GyGNkqO"
   },
   "source": [
    "### 2.2 Implement the cost and accuracy function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IgiPvgSbNuZG"
   },
   "outputs": [],
   "source": [
    "# Softmax cross entropy \n",
    "def logsumexp(Z):                                                # dimension N x C\n",
    "    Zmax = np.max(Z,axis=1)[:,None]                              # max over C\n",
    "    log_sum_exp = Zmax + np.log(np.sum(np.exp(Z - Zmax), axis=1))\n",
    "    return log_sum_exp\n",
    "\n",
    "# cost for sigmoid activation - two layers\n",
    "def cost_logistic(x, y, params):\n",
    "  Nt = x.shape[0]\n",
    "  v, w, u = params\n",
    "  b1 = np.ones((Nt,1))*0.1\n",
    "  xb = np.column_stack([x,np.ones(Nt)*0.1])\n",
    "  q1 = np.dot(xb, v) \n",
    "  z1 = logistic(np.hstack((q1,b1))) \n",
    "  q2 = np.dot(z1,w)\n",
    "  z2 = logistic(np.hstack((q2,b1)))\n",
    "  q3 = np.dot(z2, u) #N x C\n",
    "  nll = - np.mean(np.sum(q3*y, 1) - logsumexp(q3)) \n",
    "  return nll\n",
    "\n",
    "# Accuracy\n",
    "def evaluate_acc(y, yh):\n",
    "  y_pred = np.argmax(yh,axis=1)\n",
    "  accuracy = np.count_nonzero(y_pred == np.argmax(y,axis=1))/y.shape[0]\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhwM1bbNO1Fe"
   },
   "source": [
    "### 2.3. Implement the cross-validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MDnvWrREO42b"
   },
   "outputs": [],
   "source": [
    "def cross_validate(n, n_folds=6):\n",
    "    #get the number of data samples in each split\n",
    "    n_val = n // n_folds\n",
    "    inds = np.random.permutation(n)\n",
    "    inds = []\n",
    "    for f in range(n_folds):\n",
    "        tr_inds = []\n",
    "        #get the validation indexes\n",
    "        val_inds = list(range(f * n_val, (f+1)*n_val))\n",
    "        #get the train indexes\n",
    "        if f > 0:\n",
    "            tr_inds = list(range(f*n_val))\n",
    "        if f < n_folds - 1:\n",
    "            tr_inds = tr_inds + list(range((f+1)*n_val, n))\n",
    "        #The yield statement suspends function’s execution and sends a value back to the caller\n",
    "        #but retains enough state information to enable function to resume where it is left off\n",
    "        yield tr_inds, val_inds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzCYnVR2OZ7c"
   },
   "source": [
    "### 2.4 Implement the optimizer\n",
    "\n",
    "We will use a mini-batch gradient-descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1u1OikUgk3hk"
   },
   "outputs": [],
   "source": [
    "def create_mini_batch(x, y, batch_size): \n",
    "    D = x.shape[1]\n",
    "    data = np.hstack((x, y))\n",
    "    np.random.shuffle(data)\n",
    "    mini = data[:batch_size,:]                                                    \n",
    "    x_mini = mini[:,:D]\n",
    "    y_mini = mini[:,D:]\n",
    "    return x_mini, y_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AhCrG89Es0us"
   },
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "    \n",
    "    def __init__(self, learning_rate=.001, epsilon=1e-8, batch_size=100, iters=600, epochs=50, cost=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iters = iters\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.cost = cost\n",
    "        \n",
    "    def run(self, gradient_fn, x_train, y_train, x_test, y_test, params):\n",
    "      # stop the algorithm according to number of iterations and epoch\n",
    "      if self.cost == True:\n",
    "        epoch = 1\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        for epoch in range(self.epochs):\n",
    "          train_epoch_loss = []\n",
    "          test_epoch_loss = []\n",
    "          for t in range(self.iters):\n",
    "            x_mini, y_mini = create_mini_batch(x_train, y_train, self.batch_size)\n",
    "            train_loss = cost_logistic(x_mini, y_mini, params)\n",
    "            test_loss = cost_logistic(x_test, y_test, params)\n",
    "            grad = gradient_fn(x_mini, y_mini, params)\n",
    "            for p in range(len(params)):\n",
    "                params[p] -= self.learning_rate * grad[p]\n",
    "            if t % self.iters == 0:\n",
    "              print(f\"Epoch: {epoch+1}, Train error: {train_loss:.4f}, Test error: {test_loss:.4f}\")\n",
    "              epoch += 1\n",
    "            train_epoch_loss.append(train_loss)\n",
    "            test_epoch_loss.append(test_loss)\n",
    "          train_losses.append(np.mean(train_epoch_loss))\n",
    "          test_losses.append(np.mean(test_epoch_loss))\n",
    "        return params, train_losses, test_losses\n",
    "    # stop the algorithm when gradient doesn't change anymore\n",
    "      else:     \n",
    "        norms = np.array([np.inf])\n",
    "        t = 1\n",
    "        while np.any(norms > self.epsilon) and t<self.iters:\n",
    "            # create mini-batch\n",
    "            x_mini, y_mini = create_mini_batch(x_train, y_train, self.batch_size)\n",
    "            # calculate gradient for the mini-batch\n",
    "            grad = gradient_fn(x_mini, y_mini, params)\n",
    "            # update v and dw\n",
    "            for p in range(len(params)):\n",
    "                params[p] -= self.learning_rate * grad[p]\n",
    "            t += 1\n",
    "            norms = np.array([np.linalg.norm(g) for g in grad])\n",
    "        return params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlT4tqWfOqH1"
   },
   "source": [
    "## Task 3. Run the experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJ-WzYlJ39Un"
   },
   "source": [
    "### 3.1. Learning rate tuning with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "FTtvzLriPGhi",
    "outputId": "090dcff4-0101-4bbe-bf40-ab1a5f6150d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For learning rate = 0.1 and fold 0 the accuracy is: 0.11405\n",
      "For learning rate = 0.1 and fold 1 the accuracy is: 0.1141\n",
      "For learning rate = 0.1 and fold 2 the accuracy is: 0.10895\n",
      "For learning rate = 1 and fold 0 the accuracy is: 0.59055\n",
      "For learning rate = 1 and fold 1 the accuracy is: 0.5085\n",
      "For learning rate = 1 and fold 2 the accuracy is: 0.52565\n",
      "For learning rate = 10 and fold 0 the accuracy is: 0.9375\n",
      "For learning rate = 10 and fold 1 the accuracy is: 0.93615\n",
      "For learning rate = 10 and fold 2 the accuracy is: 0.9389\n"
     ]
    }
   ],
   "source": [
    "learning_list = [0.1,1,10]\n",
    "\n",
    "num_folds = 3\n",
    "acc_valid = np.zeros((len(learning_list), num_folds))\n",
    "for i, learning in enumerate(learning_list):\n",
    "    #Find the validation accuracy for num_folds splits for a given learning rate\n",
    "    for f, (tr, val) in enumerate(cross_validate(x_train.shape[0], num_folds)):\n",
    "        model = MLP2layer_logistic()\n",
    "        optimizer = GradientDescent(learning_rate=learning, epochs=100, iters=20000, cost=False)\n",
    "        model.fit(x_train[tr], y_train[tr], x_test, y_test, optimizer)\n",
    "        acc_valid[i, f] = evaluate_acc(y_train[val], model.predict(x_train[val]))\n",
    "        print('For learning rate =',learning,'and fold',f,'the accuracy is:',acc_valid[i, f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.errorbar(learning_list, np.mean(acc_valid, axis=1), np.std(acc_valid, axis=1))\n",
    "plt.xlabel('Learning rate')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For learning rate = 12 and fold 0 the accuracy is: 0.94395\n",
      "For learning rate = 12 and fold 1 the accuracy is: 0.9421\n",
      "For learning rate = 12 and fold 2 the accuracy is: 0.94615\n",
      "For learning rate = 20 and fold 0 the accuracy is: 0.9593\n",
      "For learning rate = 20 and fold 1 the accuracy is: 0.9558\n",
      "For learning rate = 20 and fold 2 the accuracy is: 0.9603\n",
      "For learning rate = 30 and fold 0 the accuracy is: 0.96735\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d2ae4ce830fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP2layer_logistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0macc_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'For learning rate ='\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'and fold'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'the accuracy is:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-97c1832521eb>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x_train, y_train, x_test, y_test, optimizer)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-99ae84fb3848>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, gradient_fn, x_train, y_train, x_test, y_test, params)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorms\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# create mini-batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mx_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0;31m# calculate gradient for the mini-batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-1b119249170a>\u001b[0m in \u001b[0;36mcreate_mini_batch\u001b[0;34m(x, y, batch_size)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mx_mini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmini\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_list = [12,20,30]\n",
    "\n",
    "num_folds = 3\n",
    "acc_valid = np.zeros((len(learning_list), num_folds))\n",
    "for i, learning in enumerate(learning_list):\n",
    "    #Find the validation accuracy for num_folds splits for a given learning rate\n",
    "    for f, (tr, val) in enumerate(cross_validate(x_train.shape[0], num_folds)):\n",
    "        model = MLP2layer_logistic()\n",
    "        optimizer = GradientDescent(learning_rate=learning, epochs=100, iters=20000, cost=False)\n",
    "        model.fit(x_train[tr], y_train[tr], x_test, y_test, optimizer)\n",
    "        acc_valid[i, f] = evaluate_acc(y_train[val], model.predict(x_train[val]))\n",
    "        print('For learning rate =',learning,'and fold',f,'the accuracy is:',acc_valid[i, f])\n",
    "        \n",
    "# The cross-validation was stopped to test more learning rates values because of the long time needed to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For learning rate = 30 and fold 0 the accuracy is: 0.96675\n",
      "For learning rate = 30 and fold 1 the accuracy is: 0.9633\n",
      "For learning rate = 30 and fold 2 the accuracy is: 0.9663\n",
      "For learning rate = 35 and fold 0 the accuracy is: 0.96775\n",
      "For learning rate = 35 and fold 1 the accuracy is: 0.96595\n",
      "For learning rate = 35 and fold 2 the accuracy is: 0.968\n",
      "For learning rate = 40 and fold 0 the accuracy is: 0.97015\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-0b97789529ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP2layer_logistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0macc_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'For learning rate ='\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'and fold'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'the accuracy is:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-97c1832521eb>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x_train, y_train, x_test, y_test, optimizer)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-99ae84fb3848>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, gradient_fn, x_train, y_train, x_test, y_test, params)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorms\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# create mini-batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mx_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0;31m# calculate gradient for the mini-batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-1b119249170a>\u001b[0m in \u001b[0;36mcreate_mini_batch\u001b[0;34m(x, y, batch_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_list = [30,35,40,50,70]\n",
    "\n",
    "num_folds = 3\n",
    "acc_valid = np.zeros((len(learning_list), num_folds))\n",
    "for i, learning in enumerate(learning_list):\n",
    "    #Find the validation accuracy for num_folds splits for a given learning rate\n",
    "    for f, (tr, val) in enumerate(cross_validate(x_train.shape[0], num_folds)):\n",
    "        model = MLP2layer_logistic()\n",
    "        optimizer = GradientDescent(learning_rate=learning, epochs=100, iters=20000, cost=False)\n",
    "        model.fit(x_train[tr], y_train[tr], x_test, y_test, optimizer)\n",
    "        acc_valid[i, f] = evaluate_acc(y_train[val], model.predict(x_train[val]))\n",
    "        print('For learning rate =',learning,'and fold',f,'the accuracy is:',acc_valid[i, f])\n",
    "        \n",
    "# The cross-validation was stopped once the best accuracy was obtained because of the long time needed to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We selected 40 as learning rate to take into account the trade-off between training time and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 96.8.\n"
     ]
    }
   ],
   "source": [
    "model = MLP2layer_logistic()\n",
    "optimizer = GradientDescent(learning_rate=40, iters = 20000, epochs=100)\n",
    "\n",
    "model.fit(x_train, y_train, x_test, y_test, optimizer)\n",
    "yh = model.predict(x_test) \n",
    "accuracy = evaluate_acc(y_test, yh)\n",
    "print(f'Accuracy is {accuracy*100:.1f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train error: 2.6274, Test error: 2.5086\n",
      "Epoch: 2, Train error: 2.0553, Test error: 2.0616\n",
      "Epoch: 3, Train error: 0.8419, Test error: 0.8345\n",
      "Epoch: 4, Train error: 0.6569, Test error: 0.5689\n",
      "Epoch: 5, Train error: 0.3781, Test error: 0.4440\n",
      "Epoch: 6, Train error: 0.3302, Test error: 0.3764\n",
      "Epoch: 7, Train error: 0.3499, Test error: 0.3293\n",
      "Epoch: 8, Train error: 0.2075, Test error: 0.2981\n",
      "Epoch: 9, Train error: 0.2652, Test error: 0.2733\n",
      "Epoch: 10, Train error: 0.2627, Test error: 0.2534\n",
      "Epoch: 11, Train error: 0.2002, Test error: 0.2357\n",
      "Epoch: 12, Train error: 0.1867, Test error: 0.2203\n",
      "Epoch: 13, Train error: 0.2377, Test error: 0.2065\n",
      "Epoch: 14, Train error: 0.1282, Test error: 0.1947\n",
      "Epoch: 15, Train error: 0.1626, Test error: 0.1873\n",
      "Epoch: 16, Train error: 0.2055, Test error: 0.1748\n",
      "Epoch: 17, Train error: 0.2415, Test error: 0.1668\n",
      "Epoch: 18, Train error: 0.1491, Test error: 0.1599\n",
      "Epoch: 19, Train error: 0.1927, Test error: 0.1546\n",
      "Epoch: 20, Train error: 0.0854, Test error: 0.1493\n",
      "Epoch: 21, Train error: 0.0814, Test error: 0.1432\n",
      "Epoch: 22, Train error: 0.1252, Test error: 0.1412\n",
      "Epoch: 23, Train error: 0.1615, Test error: 0.1360\n",
      "Epoch: 24, Train error: 0.2737, Test error: 0.1294\n",
      "Epoch: 25, Train error: 0.1506, Test error: 0.1283\n",
      "Epoch: 26, Train error: 0.0748, Test error: 0.1266\n",
      "Epoch: 27, Train error: 0.0613, Test error: 0.1209\n",
      "Epoch: 28, Train error: 0.0784, Test error: 0.1164\n",
      "Epoch: 29, Train error: 0.1158, Test error: 0.1143\n",
      "Epoch: 30, Train error: 0.0559, Test error: 0.1107\n",
      "Epoch: 31, Train error: 0.0414, Test error: 0.1101\n",
      "Epoch: 32, Train error: 0.0462, Test error: 0.1068\n",
      "Epoch: 33, Train error: 0.0815, Test error: 0.1056\n",
      "Epoch: 34, Train error: 0.0661, Test error: 0.1035\n",
      "Epoch: 35, Train error: 0.1551, Test error: 0.1044\n",
      "Epoch: 36, Train error: 0.0662, Test error: 0.0997\n",
      "Epoch: 37, Train error: 0.0854, Test error: 0.0978\n",
      "Epoch: 38, Train error: 0.0623, Test error: 0.0960\n",
      "Epoch: 39, Train error: 0.0844, Test error: 0.0985\n",
      "Epoch: 40, Train error: 0.0724, Test error: 0.0949\n",
      "Epoch: 41, Train error: 0.0242, Test error: 0.0931\n",
      "Epoch: 42, Train error: 0.0714, Test error: 0.0909\n",
      "Epoch: 43, Train error: 0.0190, Test error: 0.0914\n",
      "Epoch: 44, Train error: 0.0327, Test error: 0.0888\n",
      "Epoch: 45, Train error: 0.0437, Test error: 0.0881\n",
      "Epoch: 46, Train error: 0.1307, Test error: 0.0880\n",
      "Epoch: 47, Train error: 0.0650, Test error: 0.0862\n",
      "Epoch: 48, Train error: 0.0591, Test error: 0.0853\n",
      "Epoch: 49, Train error: 0.0230, Test error: 0.0844\n",
      "Epoch: 50, Train error: 0.0220, Test error: 0.0832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MLP2layer_logistic at 0x7f30222748d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP2layer_logistic(cost=True)\n",
    "optimizer = GradientDescent(learning_rate=40, epochs=50, iters=600, cost=True)\n",
    "\n",
    "model.fit(x_train, y_train, x_test, y_test, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(model.train_loss)), model.train_loss, '-', label='train')\n",
    "plt.plot(np.arange(len(model.test_loss)), model.test_loss, '-', label='test')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.ylim(top = 1)\n",
    "plt.savefig('epochs_2layers_log.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "COMP551_MNP3_2lay_sigm.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
