{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reverse-claim",
   "metadata": {},
   "source": [
    "# COMP 551 - Mini-project 3\n",
    "Group 63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "maritime-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.debugger import set_trace\n",
    "import scipy.sparse as sparse\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-springfield",
   "metadata": {},
   "source": [
    "## Load and preprocess the data\n",
    "\n",
    "- Load the raw data from Keras.\n",
    "- augment data\n",
    "- Vectorize 28*28 pictures to 1D vector.\n",
    "- Normalize the intensity of the pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "final-vacuum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-newcastle",
   "metadata": {},
   "source": [
    "### Creating augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "architectural-running",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating augmented dataset\n",
    "x_train_augmented = [image for image in x_train]\n",
    "y_train_augmented = [image for image in y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "furnished-behavior",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.interpolation import shift\n",
    "def shift_images(image, dx, dy):\n",
    "  image = image.reshape((28,28))\n",
    "  shifted_image = shift(image, [dy,dx], cval=0, mode=\"constant\")\n",
    "  #return shifted_image.reshape([-1])\n",
    "  return shifted_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "narrow-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dx, dy in ((1,0), (-1,0),(0,1),(0,-1)):\n",
    "  for image, label in zip(x_train, y_train):\n",
    "    x_train_augmented.append(shift_images(image, dx, dy))\n",
    "    y_train_augmented.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "amber-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_idx = np.random.permutation(len(x_train_augmented))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "stainless-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_augmented = np.array(x_train_augmented)[shuffle_idx]\n",
    "y_train_augmented = np.array(y_train_augmented)[shuffle_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daily-childhood",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_augmented.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "worldwide-radius",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300000, 784) (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train_augmented1 = np.reshape(x_train_augmented, (-1, 784)).astype('float32')\n",
    "x_test = np.reshape(x_test, (-1, 784)).astype('float32')\n",
    "print(x_train_augmented1.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sealed-bangladesh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intensity before normalization: 0.0 255.0\n",
      "Intensity after normalization: 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "print('Intensity before normalization:', np.amin(x_train_augmented1), np.amax(x_train_augmented1))\n",
    "x_train_augmented1, x_test = x_train_augmented1/255.0, x_test/255.0\n",
    "print('Intensity after normalization:', np.amin(x_train_augmented1), np.amax(x_train_augmented1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "earned-vaccine",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_augmented = keras.utils.to_categorical(y_train_augmented)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-sussex",
   "metadata": {},
   "source": [
    "## Multilayer perceptron implementation\n",
    "### Build the network\n",
    "Our task is a multiclass classification.The cost function will be the multi-class cross-entropy loss. We will use the following architecture:\n",
    "- output layer = softmax activation\n",
    "- hidden layers (0, 1 or 2): 128 units tanh activation function (it could also be ReLu or logistic function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-alarm",
   "metadata": {},
   "source": [
    "Build the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "proper-reserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic/sigmoid\n",
    "logistic = lambda z: 1./ (1 + np.exp(-z))\n",
    "\n",
    "# softmax\n",
    "eps=1e-8\n",
    "def softmax(z):\n",
    "    logits = z - np.max(z) # for numerical stability\n",
    "    sum_logits = np.sum(np.exp(logits), axis=1) +eps\n",
    "    softmax = np.exp(logits)/sum_logits[:,None] \n",
    "    return softmax\n",
    "\n",
    "# relu\n",
    "relu = lambda z: np.maximum(0,z)\n",
    "\n",
    "# derivatives of relu (formula from backprop slides)\n",
    "def relu_dv(q):\n",
    "  q[q<=0] = 0\n",
    "  q[q>0] = 1\n",
    "  return q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-march",
   "metadata": {},
   "source": [
    "Create the MLP architecture for no hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "incorporate-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    " # for no hidden layer \n",
    " class MLP_none:\n",
    "    \n",
    "    def __init__(self, M=128):\n",
    "        self.M = M\n",
    "            \n",
    "    def fit(self, x, y, optimizer):\n",
    "        N = x.shape[0]\n",
    "        C = y.shape[1] # number of classes\n",
    "        x = np.column_stack([x,np.ones(N)])\n",
    "        D = x.shape[1]\n",
    "        def gradient(x, y, w): \n",
    "            # forward pass\n",
    "            yh = softmax(np.dot(x, w))#N x C\n",
    "            # backward pass => gradient formula from class dw = (yh-y)*x\n",
    "            dy = yh - y #N x C\n",
    "            dw = np.dot(x.T, dy)/N #D x C\n",
    "            return dw\n",
    "        \n",
    "        # initialize the parameters with values in the standard normal distribution and scaled to be low\n",
    "        w = np.random.randn(D,C) * .01 #D x C\n",
    "        self.params = optimizer.run(gradient, x, y, w)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        w = self.params\n",
    "        Nt = x.shape[0]\n",
    "        x = np.column_stack([x,np.ones(Nt)])\n",
    "        yh = softmax(np.dot(x, w))#N x C\n",
    "        return yh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-amplifier",
   "metadata": {},
   "source": [
    "Create the MLP architecture for one hidden layer with ReLu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "higher-savage",
   "metadata": {},
   "outputs": [],
   "source": [
    " # for 1 hidden layer with relu activation\n",
    " class MLP_relu:\n",
    "    \n",
    "    def __init__(self, M = 128):\n",
    "        self.M = M\n",
    "            \n",
    "    def fit(self, x, y, optimizer):\n",
    "        N = x.shape[0]\n",
    "        C = y.shape[1] # number of classes\n",
    "        # add bias to the input layer\n",
    "        x = np.column_stack([x,np.ones(N)])\n",
    "        D = x.shape[1]\n",
    "        def gradient(x, y, params):\n",
    "            v, w = params\n",
    "            # forward pass\n",
    "            q = np.dot(x, v)\n",
    "            z = relu(q) #N x M\n",
    "            yh = softmax(np.dot(z, w))#N x C\n",
    "            # backward pass => gradient formula adapted from class dw = (yh-y)*z, dv = (yh-y)*w*deriv_relu(q)*x\n",
    "            dy = yh - y #N x C\n",
    "            dw = np.dot(z.T, dy)/N #M x C\n",
    "            dz = np.dot(dy, w.T) #N x M\n",
    "            dq = relu_dv(q)\n",
    "            dv = np.dot(x.T, dz * dq)/N #D x M\n",
    "            dparams = [dv, dw]\n",
    "            return dparams\n",
    "        \n",
    "        # initialize the parameters with values in the standard normal distribution and scaled to be low\n",
    "        w = np.random.randn(self.M,C) * .01 #M x C\n",
    "        v = np.random.randn(D,self.M) * .01 #D x M\n",
    "        params0 = [v,w]\n",
    "\n",
    "        # run the mini-batch gradient descent to update the parameters\n",
    "        self.params = optimizer.run(gradient, x, y, params0)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        v, w = self.params\n",
    "        # add bias to the input layer\n",
    "        Nt = x.shape[0]\n",
    "        x = np.column_stack([x,np.ones(Nt)])\n",
    "        # forward pass only using updated parameters\n",
    "        z = relu(np.dot(x, v)) #N x M\n",
    "        yh = softmax(np.dot(z, w))#N x C\n",
    "        return yh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-prerequisite",
   "metadata": {},
   "source": [
    "Create the MLP architecture for two hidden layer with ReLu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eligible-melissa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 2 hidden layer with relu activation\n",
    "class MLP2layer_relu:\n",
    "    \n",
    "    def __init__(self, M = 128):\n",
    "        self.M = M\n",
    "            \n",
    "    def fit(self, x, y, optimizer):\n",
    "        N = x.shape[0]\n",
    "        C = y.shape[1] # number of classes\n",
    "        # add bias to the input layer\n",
    "        x = np.column_stack([x,np.ones(N)*0.1])\n",
    "        D = x.shape[1]\n",
    "        def gradient(x, y, params):\n",
    "            v, w, u = params\n",
    "            # forward pass\n",
    "            n = x.shape[0]\n",
    "            b = np.ones((n,1))*0.1\n",
    "\n",
    "            q1 = np.dot(x, v) #np.column_stack([np.dot(x, v),np.ones(N)*0.1]) #trying adding bias here\n",
    "            z1 = relu(np.hstack((q1,b))) #N x M want to column stack to add bias here\n",
    "            q2 = np.dot(z1,w) #np.column_stack([np.dot(z1,w),np.ones(N)*0.1]) #trying adding bias here\n",
    "            z2 = relu(np.hstack((q2,b)))\n",
    "            yh = softmax(np.dot(z2, u))#N x C\n",
    "            # backward pass => gradient formula adapted from class dw = (yh-y)*z, dv = (yh-y)*w*deriv_relu(q)*x\n",
    "            \n",
    "            dy = yh - y #N x C\n",
    "            \n",
    "            du = np.dot(z2.T,dy)/N\n",
    "            \n",
    "            #print(dz2.shape)  \n",
    "            dz2 = np.dot(dy,u.T)\n",
    "            dz2 = np.delete(dz2, -1, axis=1)\n",
    "            dq2 = relu_dv(q2)\n",
    "            #print(dz2.T * dq2)\n",
    "            dw = np.dot(z1.T, dz2 * dq2)/N #M x C\n",
    "            dz1 = np.dot(dz2, w.T) #N x M\n",
    "            dz1 = np.delete(dz1,-1,axis=1)\n",
    "            dq1 = relu_dv(q1)\n",
    "            dv = np.dot(x.T, dz1 * dq1)/N #D x M\n",
    "            dparams = [dv, dw, du]\n",
    "            return dparams\n",
    "        \n",
    "        # initialize the parameters with values in the standard normal distribution and scaled to be low\n",
    "        u = np.random.randn(self.M+1,C) * 0.1 #M x C\n",
    "        w = np.random.randn(self.M+1,self.M) * .01 #M x M\n",
    "        v = np.random.randn(D,self.M) * .01 #D x M\n",
    "        \n",
    "        params0 = [v,w,u]\n",
    "\n",
    "        # run the mini-batch gradient descent to update the parameters\n",
    "        self.params = optimizer.run(gradient, x, y, params0)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        v, w, u = self.params\n",
    "        # add bias to the input layer\n",
    "        Nt = x.shape[0]\n",
    "        x = np.column_stack([x,np.ones(Nt)*0.1])\n",
    "        b1 = np.ones((Nt,1))*0.1\n",
    "     \n",
    "        # forward pass only using updated parameters\n",
    "\n",
    "        q1 = np.dot(x,v)\n",
    "        z1 = relu(np.hstack((q1,b1)))\n",
    "        q2 = np.dot(z1,w)\n",
    "        z2 = relu(np.hstack((q2,b1)))\n",
    "        yh = softmax(np.dot(z2, u))#N x C\n",
    "        return yh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-parts",
   "metadata": {},
   "source": [
    "### Implement the cost and accuracy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "closed-promise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax cross entropy \n",
    "def logsumexp(Z):                                                # dimension C x N\n",
    "    Zmax = np.max(Z,axis=0)[None,:]                              # max over C\n",
    "    log_sum_exp = Zmax + np.log(np.sum(np.exp(Z - Zmax), axis=0))\n",
    "    return log_sum_exp\n",
    "\n",
    "# cost for relu activation\n",
    "def cost_relu(x, #N x D \n",
    "         y, #N x C \n",
    "         w, #M x C \n",
    "         v, #D x M\n",
    "        ):\n",
    "  q = np.dot(x, v) #N x M\n",
    "  z = relu(q) #N x M\n",
    "  u = np.dot(z, w) #N x C\n",
    "  yh = softmax(u)\n",
    "  nll = - np.mean(np.sum(u*y, 1) - logsumexp(u)) \n",
    "  return nll\n",
    "\n",
    "# Accuracy\n",
    "def evaluate_acc(y, yh):\n",
    "  y_pred = np.argmax(yh,axis=1)\n",
    "  accuracy = np.count_nonzero(y_pred == np.argmax(y,axis=1))/y.shape[0]\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-handy",
   "metadata": {},
   "source": [
    "### Implement the optimizer\n",
    "\n",
    "We will use a mini-batch gradient-descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "quiet-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(x, y, batch_size): \n",
    "    D = x.shape[1]\n",
    "    data = np.hstack((x, y))\n",
    "    np.random.shuffle(data)\n",
    "    mini = data[:batch_size,:]                                                    \n",
    "    x_mini = mini[:,:D]\n",
    "    y_mini = mini[:,D:]\n",
    "    return x_mini, y_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "improving-present",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "    \n",
    "    def __init__(self, learning_rate=.001, max_iters=1e4, epsilon=1e-8, batch_size=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iters = max_iters\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def run(self, gradient_fn, x, y, params):\n",
    "        norms = np.array([np.inf])\n",
    "        t = 1\n",
    "        while np.any(norms > self.epsilon) and t < self.max_iters:\n",
    "            # create mini-batch\n",
    "            x_mini, y_mini = create_mini_batch(x, y, self.batch_size)\n",
    "            # calculate gradient for the mini-batch\n",
    "            grad = gradient_fn(x_mini, y_mini, params)\n",
    "            # update v and dw\n",
    "            for p in range(len(params)):\n",
    "                params[p] -= self.learning_rate * grad[p]\n",
    "            t += 1\n",
    "            norms = np.array([np.linalg.norm(g) for g in grad])\n",
    "        return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "veterinary-pattern",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent_none:\n",
    "    \n",
    "    def __init__(self, learning_rate=.001, max_iters=1e4, epsilon=1e-8, batch_size=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iters = max_iters\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def run(self, gradient_fn, x, y, params):\n",
    "        norms = np.array([np.inf])\n",
    "        t = 1\n",
    "        while np.any(norms > self.epsilon) and t < self.max_iters:\n",
    "            x_mini, y_mini = create_mini_batch(x, y, self.batch_size)\n",
    "            grad = gradient_fn(x_mini, y_mini, params)\n",
    "            params -= self.learning_rate * grad\n",
    "            t += 1\n",
    "            norms = np.array([np.linalg.norm(g) for g in grad])\n",
    "        return params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-selling",
   "metadata": {},
   "source": [
    "## Run the experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-array",
   "metadata": {},
   "source": [
    "Test MLP with no hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "single-recorder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n",
      "[[0.09966634 0.05674332 0.08324794 0.08729533 0.09625264 0.07304213\n",
      "  0.0706631  0.21512254 0.0894713  0.12849534]\n",
      " [0.11673741 0.10468862 0.13776297 0.13645928 0.05004836 0.1013022\n",
      "  0.12139154 0.05088396 0.12218948 0.05853616]\n",
      " [0.06807997 0.18914192 0.10497551 0.10542814 0.07864706 0.0808471\n",
      "  0.09236405 0.08733863 0.1044884  0.0886892 ]]\n",
      "Accuracy is 75.4.\n"
     ]
    }
   ],
   "source": [
    "model = MLP_none()\n",
    "optimizer = GradientDescent_none(learning_rate=.1, max_iters=20000)\n",
    "\n",
    "model.fit(x_train_augmented1, y_train_augmented, optimizer)\n",
    "yh = model.predict(x_test) \n",
    "print(yh.shape)\n",
    "print(yh[:3,])\n",
    "accuracy = evaluate_acc(y_test, yh)\n",
    "print(f'Accuracy is {accuracy*100:.1f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-prior",
   "metadata": {},
   "source": [
    "Test MLP with one hidden layer for ReLu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "democratic-dallas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n",
      "[[2.09509455e-05 4.09279205e-09 3.73911328e-04 4.95574179e-03\n",
      "  6.87811722e-10 1.60439263e-05 3.31668917e-11 9.92348567e-01\n",
      "  9.11942778e-06 4.65373321e-05]\n",
      " [5.92817122e-04 1.01666044e-03 8.82237191e-01 2.46249257e-02\n",
      "  2.66581757e-07 1.29132905e-03 8.39740985e-03 2.17716040e-08\n",
      "  9.91867270e-03 4.35509703e-08]\n",
      " [9.99548098e-07 6.55654999e-01 2.52778219e-03 3.71767921e-04\n",
      "  1.15981043e-03 5.84105696e-05 3.82219255e-03 2.43303196e-03\n",
      "  7.28347919e-04 6.13814982e-05]]\n",
      "Accuracy is 93.9.\n"
     ]
    }
   ],
   "source": [
    "model = MLP_relu()\n",
    "optimizer = GradientDescent(learning_rate=2, max_iters=20000)\n",
    "\n",
    "model.fit(x_train_augmented1, y_train_augmented, optimizer)\n",
    "yh = model.predict(x_test) \n",
    "print(yh.shape)\n",
    "print(yh[:3,])\n",
    "accuracy = evaluate_acc(y_test, yh)\n",
    "print(f'Accuracy is {accuracy*100:.1f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-immune",
   "metadata": {},
   "source": [
    "Test MLP with two hidden layer for ReLu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "wrong-officer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n",
      "[[1.03667292e-03 7.44680901e-04 7.57676393e-04 9.08588440e-03\n",
      " 2.91209439e-03 1.53216466e-03 2.99343052e-05 9.14802823e-01\n",
      " 4.83905104e-03 6.42567058e-02]\n",
      "[4.24931465e-02 4.42952453e-03 6.66883630e-01 8.45419289e-02\n",
      " 1.82487462e-04 4.93942852e-02 9.82304308e-02 1.67124401e-05\n",
      " 5.37333287e-02 8.86373658e-05]\n",
      "[3.56114690e-04 8.94878568e-01 3.05090741e-02 1.80016065e-02\n",
      " 3.51352037e-03 2.40707183e-03 7.95288951e-03 9.57534981e-03\n",
      " 2.58602401e-02 6.94000582e-03]]\n",
      "Accuracy is 96.7.\n"
     ]
    }
   ],
   "source": [
    "model = MLP2layer_relu()\n",
    "optimizer = GradientDescent(learning_rate=2, max_iters=20000)\n",
    "\n",
    "model.fit(x_train_augmented1, y_train_augmented, optimizer)\n",
    "yh = model.predict(x_test) \n",
    "print(yh.shape)\n",
    "print(yh[:3,])\n",
    "accuracy = evaluate_acc(y_test, yh)\n",
    "print(f'Accuracy is {accuracy*100:.1f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "challenging-principle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y, yh):\n",
    "    n_classes = y.shape[1]\n",
    "    c_matrix = np.zeros((n_classes, n_classes))\n",
    "    for c1 in range(n_classes):\n",
    "        for c2 in range(n_classes):\n",
    "        #(y==c1)*(yh==c2) is 1 when both conditions are true or 0\n",
    "            c_matrix[c1, c2] = np.sum((np.argmax(y,axis=1)==c1)*(np.argmax(yh,axis=1)==c2))\n",
    "    return c_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "varied-screening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 939.    0.    4.    2.    0.   25.    6.    1.    3.    0.]\n",
      " [   0. 1104.    5.    5.    0.    0.    3.    0.   17.    1.]\n",
      " [  22.   36.  856.   23.   21.    2.   28.    9.   32.    3.]\n",
      " [   5.    5.   26.  878.    1.   36.    1.   21.   30.    7.]\n",
      " [   2.    8.    4.    0.  824.    0.   24.    1.    4.  115.]\n",
      " [  30.    4.   12.   97.   19.  603.   38.   14.   62.   13.]\n",
      " [  27.    3.   19.    1.   19.   15.  871.    0.    3.    0.]\n",
      " [   4.   41.   21.    1.    7.    1.    0.  882.    7.   64.]\n",
      " [   7.   22.   12.   44.   14.   69.   24.   13.  744.   25.]\n",
      " [   9.    9.    4.   10.  130.   15.    0.   53.   12.  767.]]\n"
     ]
    }
   ],
   "source": [
    "cmat = confusion_matrix(y_test, yh)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(cmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-ultimate",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
