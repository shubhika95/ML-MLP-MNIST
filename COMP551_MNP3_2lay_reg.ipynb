{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMP551_MNP3_2lay_reg.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6331ZSsQGY3"
      },
      "source": [
        "# COMP 551 - Mini-project 3\n",
        "Group 63"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifel1K0gBWt_"
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "%matplotlib inline                                 \n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.debugger import set_trace  \n",
        "import scipy.sparse as sparse\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import os\n",
        "from google.colab import files\n",
        "np.random.seed(1234)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eNcbfC91k8E",
        "outputId": "dbd5163b-9db8-4810-b148-aeb30c08d0c3"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "\"\"\"Vectorize the 28*28 pictures to a 784 vector.\"\"\"\n",
        "\n",
        "x_train = np.reshape(x_train, (-1, 784)).astype('float32')\n",
        "x_test = np.reshape(x_test, (-1, 784)).astype('float32')\n",
        "\n",
        "\n",
        "\"\"\"The intensity ranges from 0 to 255. We divide all intensities by the maximum (255) to obtain a [0-1] range.\"\"\"\n",
        "\n",
        "\n",
        "x_train, x_test = x_train/255.0, x_test/255.0\n",
        "\n",
        "\n",
        "\"\"\"We transform the (N,) vector of labels using one-hot encoding into a (N,C) matrix.\"\"\"\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train)\n",
        "y_test = keras.utils.to_categorical(y_test)\n",
        "\n",
        "#this is just because I was using these values on my local computer and had errors with installing tensorflor so I could install keras \n",
        "np.save('x_train.npy',x_train)\n",
        "np.save('x_test.npy',x_test)\n",
        "np.save('y_train.npy',y_train)\n",
        "np.save('y_test.npy',y_test)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28) (10000, 28, 28) (60000,) (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b08Mmbs8lr81"
      },
      "source": [
        "## Task 1. Data pre-processing\n",
        "\n",
        "- Load the raw data from Keras.\n",
        "- Vectorize 28*28 pictures to 1D vector.\n",
        "- Normalize the intensity of the pixel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZyGXlaKojgz"
      },
      "source": [
        "Load the MNIST dataset distributed with Keras. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N01fKjwJDKAz"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rX8l1wVDOiN"
      },
      "source": [
        "Vectorize the 28*28 pictures to a 784 vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4gvmJN6DPxI"
      },
      "source": [
        "x_train = np.reshape(x_train, (-1, 784)).astype('float32')\n",
        "x_test = np.reshape(x_test, (-1, 784)).astype('float32')\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQqnKrdkEcw6"
      },
      "source": [
        "The intensity ranges from 0 to 255. We divide all intensities by the maximum (255) to obtain a [0-1] range."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-s4iFV3Ez3H",
        "outputId": "11c58570-dc0e-4edb-84a6-e05dc2ea4ef1"
      },
      "source": [
        "print('Intensity before normalization:', np.amin(x_train), np.amax(x_train))\n",
        "x_train, x_test = x_train/255.0, x_test/255.0\n",
        "print('Intensity after normalization:', np.amin(x_train), np.amax(x_train))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Intensity before normalization: 0.0 255.0\n",
            "Intensity after normalization: 0.0 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUnquDPg1D-k"
      },
      "source": [
        "We transform the (N,) vector of labels using one-hot encoding into a (N,C) matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fXA7QipDn4Z"
      },
      "source": [
        "y_train = keras.utils.to_categorical(y_train)\n",
        "y_test = keras.utils.to_categorical(y_test)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m3fZmPK14rd"
      },
      "source": [
        "Subset the data to use in Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fx0N2a_Q1hMO"
      },
      "source": [
        "#when running in colab I subsetted the data\n",
        "#data_slice = 3000\n",
        "#x_train = x_train[:data_slice,:]\n",
        "#y_train = y_train[:data_slice,:]\n",
        "#x_test = x_test[:data_slice,:]\n",
        "#y_test = y_test[:data_slice,:]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpKU5kZOJkEe"
      },
      "source": [
        "## Task 2. Multilayer perceptron implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6FNYQoRs0ue"
      },
      "source": [
        "### 2.1 Build the network\n",
        "Our task is a multiclass classification.The cost function will be the multi-class cross-entropy loss. We will use the following architecture:\n",
        "- output layer = softmax activation\n",
        "- hidden layers (0, 1 or 2): 128 units, ReLu or logistic activation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUW6HFnH43cv"
      },
      "source": [
        "First, we implement the activation functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1skqPZ_s0uq"
      },
      "source": [
        "# logistic/sigmoid\n",
        "logistic = lambda z: 1./ (1 + np.exp(-z))\n",
        "\n",
        "# softmax\n",
        "eps=1e-8\n",
        "def softmax(z):\n",
        "    logits = z - np.max(z) # for numerical stability\n",
        "    sum_logits = np.sum(np.exp(logits), axis=1) +eps\n",
        "    softmax = np.exp(logits)/sum_logits[:,None] \n",
        "    return softmax\n",
        "\n",
        "# relu\n",
        "relu = lambda z: np.maximum(0,z)\n",
        "\n",
        "# derivatives of relu (formula from backprop slides)\n",
        "def relu_dv(q):\n",
        "  q[q<=0] = 0\n",
        "  q[q>0] = 1\n",
        "  return q"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxFKbS58_69S"
      },
      "source": [
        "Next, we build the MLP class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKIoaax3f6f0"
      },
      "source": [
        "# for 2 hidden layer with relu activation\n",
        "class MLP2layer_relu_l1:\n",
        "    \n",
        "    def __init__(self, M = 128,lambd=0.005):\n",
        "        self.M = M\n",
        "        self.lambd=lambd\n",
        "            \n",
        "    def fit(self, x_train, y_train, x_test, y_test, optimizer):\n",
        "        N = x_train.shape[0]\n",
        "        C = y_train.shape[1] # number of classes\n",
        "        D = x_train.shape[1]\n",
        "        def gradient(x, y, params,lmbd):\n",
        "            v, w, u = params\n",
        "            # forward pass\n",
        "            N = x.shape[0]\n",
        "            # add bias to the input layer\n",
        "            x = np.column_stack([x,np.ones(N)*0.1])\n",
        "            b = np.ones((N,1))*0.1\n",
        "\n",
        "            q1 = np.dot(x, v) \n",
        "            z1 = relu(np.hstack((q1,b))) #N x M want to column stack to add bias here\n",
        "            q2 = np.dot(z1,w) \n",
        "            z2 = relu(np.hstack((q2,b)))\n",
        "            yh = softmax(np.dot(z2, u))#N x C\n",
        "            # backward pass => gradient formula adapted from class dw = (yh-y)*z, dv = (yh-y)*w*deriv_relu(q)*x\n",
        "            \n",
        "            dy = yh - y #N x C\n",
        "            \n",
        "            du = np.dot(z2.T,dy)/N \n",
        "            du[:-1] = du[:-1] + lmbd*np.sign(u[:-1]) #/N not adding to biases\n",
        "            \n",
        "            dz2 = np.dot(dy,u.T)\n",
        "            dz2 = np.delete(dz2, -1, axis=1)\n",
        "            dq2 = relu_dv(q2)\n",
        "            \n",
        "            dw = np.dot(z1.T, dz2 * dq2)/N \n",
        "            dw[:-1] = dw[:-1] + lmbd*np.sign(w[:-1]) #/N not adding to biases\n",
        "            dz1 = np.dot(dz2, w.T) #N x M\n",
        "            dz1 = np.delete(dz1,-1,axis=1)\n",
        "            dq1 = relu_dv(q1)\n",
        "            dv = np.dot(x.T, dz1 * dq1)/N\n",
        "            dv[:-1] = dv[:-1] + lmbd*np.sign(v[:-1]) #/N not adding to biases\n",
        "            dparams = [dv, dw, du]\n",
        "            return dparams\n",
        "        \n",
        "        # initialize the parameters with values in the standard normal distribution and scaled to be low\n",
        "        u = np.random.randn(self.M+1,C) * 0.1 #M x C\n",
        "        w = np.random.randn(self.M+1,self.M) * .01 #M x M\n",
        "        v = np.random.randn(D+1,self.M) * .01 #D x M\n",
        "        \n",
        "        params0 = [v,w,u]\n",
        "\n",
        "        # run the mini-batch gradient descent to update the parameters\n",
        "        self.params, self.train_loss, self.test_loss = optimizer.run(gradient, x_train, y_train, x_test, y_test, params0,self.lambd,reg='l1')\n",
        "        return self\n",
        "    \n",
        "    def predict(self, x):\n",
        "        v, w, u = self.params\n",
        "        # add bias to the input layer\n",
        "        Nt = x.shape[0]\n",
        "        x = np.column_stack([x,np.ones(Nt)*0.1])\n",
        "        b1 = np.ones((Nt,1))*0.1\n",
        "     \n",
        "        # forward pass only using updated parameters\n",
        "\n",
        "        q1 = np.dot(x,v)\n",
        "        z1 = relu(np.hstack((q1,b1)))\n",
        "        q2 = np.dot(z1,w)\n",
        "        z2 = relu(np.hstack((q2,b1)))\n",
        "        yh = softmax(np.dot(z2, u))#N x C\n",
        "        return yh"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBaZYtwLpoGv"
      },
      "source": [
        "# for 2 hidden layer with relu activation\n",
        "class MLP2layer_relu_l2:\n",
        "    \n",
        "    def __init__(self, M = 128,lambd=0.005):\n",
        "        self.M = M\n",
        "        self.lambd=lambd\n",
        "            \n",
        "    def fit(self, x_train, y_train, x_test, y_test, optimizer):\n",
        "        N = x_train.shape[0]\n",
        "        C = y_train.shape[1] # number of classes\n",
        "        D = x_train.shape[1]\n",
        "        def gradient(x, y, params,lmbd):\n",
        "            v, w, u = params\n",
        "            # forward pass\n",
        "            N = x.shape[0]\n",
        "            # add bias to the input layer\n",
        "            x = np.column_stack([x,np.ones(N)*0.1])\n",
        "            b = np.ones((N,1))*0.1\n",
        "\n",
        "            q1 = np.dot(x, v) #np.column_stack([np.dot(x, v),np.ones(N)*0.1]) #trying adding bias here\n",
        "            z1 = relu(np.hstack((q1,b))) #N x M want to column stack to add bias here\n",
        "            q2 = np.dot(z1,w) #np.column_stack([np.dot(z1,w),np.ones(N)*0.1]) #trying adding bias here\n",
        "            z2 = relu(np.hstack((q2,b)))\n",
        "            yh = softmax(np.dot(z2, u))#N x C\n",
        "            # backward pass => gradient formula adapted from class dw = (yh-y)*z, dv = (yh-y)*w*deriv_relu(q)*x\n",
        "            \n",
        "            dy = yh - y #N x C\n",
        "            \n",
        "            du = np.dot(z2.T,dy)/N \n",
        "            #not adding to biases\n",
        "            du[:-1] = du[:-1] + lmbd*u[:-1] #/N when dividing lambda term by N it was leading to lots of weights being converted to NaNs\n",
        " \n",
        "            dz2 = np.dot(dy,u.T)\n",
        "            dz2 = np.delete(dz2, -1, axis=1)\n",
        "            dq2 = relu_dv(q2)\n",
        "   \n",
        "            dw = np.dot(z1.T, dz2 * dq2)/N \n",
        "            #not adding to biases\n",
        "            dw[:-1] = dw[:-1] + lmbd*w[:-1] #/N #M x C\n",
        "            dz1 = np.dot(dz2, w.T) #N x M\n",
        "            dz1 = np.delete(dz1,-1,axis=1)\n",
        "            dq1 = relu_dv(q1)\n",
        "            dv = np.dot(x.T, dz1 * dq1)/N\n",
        "            #not adding to biases\n",
        "            dv[:-1] = dv[:-1] + lmbd*v[:-1] #/N #D x M\n",
        "            dparams = [dv, dw, du]\n",
        "            return dparams\n",
        "        \n",
        "        # initialize the parameters with values in the standard normal distribution and scaled to be low\n",
        "        u = np.random.randn(self.M+1,C) * 0.1 #M x C\n",
        "        w = np.random.randn(self.M+1,self.M) * .01 #M x M\n",
        "        v = np.random.randn(D+1,self.M) * .01 #D x M\n",
        "        \n",
        "        params0 = [v,w,u]\n",
        "\n",
        "        # run the mini-batch gradient descent to update the parameters\n",
        "        self.params, self.train_loss, self.test_loss = optimizer.run(gradient, x_train, y_train, x_test, y_test, params0,self.lambd,reg='l1')\n",
        "        return self\n",
        "    \n",
        "    def predict(self, x):\n",
        "        v, w, u = self.params\n",
        "        # add bias to the input layer\n",
        "        Nt = x.shape[0]\n",
        "        x = np.column_stack([x,np.ones(Nt)*0.1])\n",
        "        b1 = np.ones((Nt,1))*0.1\n",
        "     \n",
        "        # forward pass only using updated parameters\n",
        "\n",
        "        q1 = np.dot(x,v)\n",
        "        z1 = relu(np.hstack((q1,b1)))\n",
        "        q2 = np.dot(z1,w)\n",
        "        z2 = relu(np.hstack((q2,b1)))\n",
        "        yh = softmax(np.dot(z2, u))#N x C\n",
        "        return yh"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65sh_GyGNkqO"
      },
      "source": [
        "### 2.2 Implement the cost and accuracy function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgiPvgSbNuZG"
      },
      "source": [
        "# Softmax cross entropy \n",
        "def logsumexp(Z):                                                # dimension N x C\n",
        "    Zmax = np.max(Z,axis=1)[:,None]                              # max over C\n",
        "    log_sum_exp = Zmax + np.log(np.sum(np.exp(Z - Zmax), axis=1))\n",
        "    return log_sum_exp\n",
        "\n",
        "# cost for relu activation - two layers\n",
        "def cost_relu(x, y, params):\n",
        "  Nt = x.shape[0]\n",
        "  v, w, u = params\n",
        "  b1 = np.ones((Nt,1))*0.1\n",
        "  xb = np.column_stack([x,np.ones(Nt)*0.1])\n",
        "  q1 = np.dot(xb, v) \n",
        "  z1 = relu(np.hstack((q1,b1))) \n",
        "  q2 = np.dot(z1,w)\n",
        "  z2 = relu(np.hstack((q2,b1)))\n",
        "  q3 = np.dot(z2, u) #N x C\n",
        "  nll = - np.mean(np.sum(q3*y, 1) - logsumexp(q3)) \n",
        "  return nll\n",
        "\n",
        "# cost for relu activation - two layers with l2 regularization\n",
        "def cost_relu_l2(x, y, params,lmbd):\n",
        "  Nt = x.shape[0]\n",
        "  v,w,u = params\n",
        "  l2reg_cost = lmbd*(np.mean(np.square(v))+np.mean(np.square(w))+np.mean(np.square(v)))/(2) #used mean of each rather then total dimension value because with the biases v,w, and u aren't all the same size\n",
        "  cost_reg = cost_relu(x,y,params) + l2reg_cost\n",
        "  return cost_reg\n",
        "\n",
        "# cost for relu activation - two layers with l2 regularization\n",
        "def cost_relu_l1(x, y, params,lmbd):\n",
        "  Nt = x.shape[1]\n",
        "  v,w,u = params\n",
        "  l1reg_cost = lmbd*(np.mean(np.abs(v))+np.mean(np.abs(w))+np.mean(np.abs(v)))/(2) #used mean of each rather then total dimension value because with the biases v,w, and u aren't all the same size\n",
        "  cost_reg = cost_relu(x,y,params) + l1reg_cost\n",
        "  return cost_reg\n",
        "\n",
        "# Accuracy\n",
        "def evaluate_acc(y, yh):\n",
        "  y_pred = np.argmax(yh,axis=1)\n",
        "  accuracy = np.count_nonzero(y_pred == np.argmax(y,axis=1))/y.shape[0]\n",
        "  return accuracy"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzCYnVR2OZ7c"
      },
      "source": [
        "### 2.3 Implement the optimizer\n",
        "\n",
        "We will use a mini-batch gradient-descent algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u1OikUgk3hk"
      },
      "source": [
        "def create_mini_batch(x, y, batch_size): \n",
        "    D = x.shape[1]\n",
        "    data = np.hstack((x, y))\n",
        "    np.random.shuffle(data)\n",
        "    mini = data[:batch_size,:]                                                    \n",
        "    x_mini = mini[:,:D]\n",
        "    y_mini = mini[:,D:]\n",
        "    return x_mini, y_mini"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhCrG89Es0us"
      },
      "source": [
        "class GradientDescent:\n",
        "    \n",
        "    def __init__(self, learning_rate=.001, epsilon=1e-8, batch_size=100, iters=600, epochs=50):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.iters = iters\n",
        "        self.epsilon = epsilon\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        \n",
        "    def run(self, gradient_fn, x_train, y_train, x_test, y_test, params, lmbd, reg):\n",
        "        epoch = 1\n",
        "        train_losses = []\n",
        "        test_losses = []\n",
        "        for epoch in range(self.epochs):\n",
        "          train_epoch_loss = []\n",
        "          test_epoch_loss = []\n",
        "          for t in range(self.iters):\n",
        "            x_mini, y_mini = create_mini_batch(x_train, y_train, self.batch_size)\n",
        "\n",
        "            if reg == 'l1':\n",
        "              train_loss = cost_relu_l1(x_mini, y_mini, params,lmbd)\n",
        "              test_loss = cost_relu_l1(x_test, y_test, params,lmbd)  \n",
        "            elif reg == 'l2':\n",
        "              train_loss = cost_relu_l2(x_mini, y_mini, params,lmbd)\n",
        "              test_loss = cost_relu_l2(x_test, y_test, params,lmbd)\n",
        "            else:\n",
        "              lmbd = 0\n",
        "            grad = gradient_fn(x_mini, y_mini, params,lmbd)\n",
        "            for p in range(len(params)):\n",
        "                params[p] -= self.learning_rate * grad[p]\n",
        "            if t % self.iters == 0:\n",
        "              print(f\"Epoch: {epoch+1}, Train error: {train_loss:.4f}, Test error: {test_loss:.4f}\")\n",
        "              epoch += 1\n",
        "            train_epoch_loss.append(train_loss)\n",
        "            test_epoch_loss.append(test_loss)\n",
        "          train_losses.append(np.mean(train_epoch_loss))\n",
        "          test_losses.append(np.mean(test_epoch_loss))\n",
        "        return params, train_losses, test_losses\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlT4tqWfOqH1"
      },
      "source": [
        "## Task 3. Run the experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ-WzYlJ39Un"
      },
      "source": [
        "Model with 2 hidden layers and ReLu activation and L1 regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "fpFMcaVC36iu",
        "outputId": "a40f0f3b-13bb-4b71-9cf7-0b6011b989bf"
      },
      "source": [
        "model = MLP2layer_relu_l1(lambd=0.005)\n",
        "optimizer = GradientDescent(learning_rate=0.1, epochs=20,batch_size=100)\n",
        "\n",
        "model.fit(x_train, y_train, x_test, y_test, optimizer)\n",
        "\n",
        "plt.plot(np.arange(len(model.train_loss)), model.train_loss, '-', label='train')\n",
        "plt.plot(np.arange(len(model.test_loss)), model.test_loss, '-', label='test')\n",
        "plt.legend()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Train error: 2.3035, Test error: 2.3028\n",
            "Epoch: 2, Train error: 2.3025, Test error: 2.3025\n",
            "Epoch: 3, Train error: 2.3006, Test error: 2.3023\n",
            "Epoch: 4, Train error: 2.3039, Test error: 2.3022\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-0f1bfaacb4f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-920274e5bfde>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x_train, y_train, x_test, y_test, optimizer)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# run the mini-batch gradient descent to update the parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-385ffcea55ec>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, gradient_fn, x_train, y_train, x_test, y_test, params, lmbd, reg)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m               \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_relu_l1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlmbd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m               \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_relu_l1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlmbd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mreg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l2'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m               \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_relu_l2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlmbd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-ad0b6da6fc0a>\u001b[0m in \u001b[0;36mcost_relu_l1\u001b[0;34m(x, y, params, lmbd)\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0ml1reg_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlmbd\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m   \u001b[0mcost_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml1reg_cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcost_reg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-ad0b6da6fc0a>\u001b[0m in \u001b[0;36mcost_relu\u001b[0;34m(x, y, params)\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mq1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mq2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0mz2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mq3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#N x C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKaNvUQyPb2c"
      },
      "source": [
        "yh = model.predict(x_test) \n",
        "accuracy = evaluate_acc(y_test, yh)\n",
        "print(f'Accuracy is {accuracy*100:.1f}.')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImGRxXIbASld",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "outputId": "47e35366-2145-4858-c2f2-13bbad4d9b2a"
      },
      "source": [
        "lamb_list = [0.004,0.005,0.007,0.01,0.05]\n",
        "optimizer = GradientDescent(learning_rate=0.1, epochs=10, iters=1000)\n",
        "acc_list = []\n",
        "for i,l in enumerate(lamb_list):\n",
        "  model = MLP2layer_relu_l2(lambd=l)\n",
        "  model.fit(x_train, y_train, x_test, y_test, optimizer)\n",
        "  yh = model.predict(x_test) \n",
        "  accs = evaluate_acc(y_test, yh)\n",
        "  acc_list.append(accs)\n",
        "  print(f'Accuracy is {accs*100:.1f}.')\n",
        "  print('for lamda='+str(l))\n",
        "plt.plot(lamb_list, acc_list)\n",
        "#plt.legend()\n",
        "plt.xlabel('Lambda')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.savefig('cv_reg_changelam.png')\n",
        "plt.show()\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Train error: 2.3014, Test error: 2.3029\n",
            "Epoch: 2, Train error: 0.0608, Test error: 0.3883\n",
            "Epoch: 3, Train error: 0.0715, Test error: 0.3506\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:160: RuntimeWarning: overflow encountered in reduce\n",
            "  ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in multiply\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in subtract\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in subtract\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 4, Train error: nan, Test error: nan\n",
            "Epoch: 5, Train error: nan, Test error: nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-025b8c1344ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlamb_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP2layer_relu_l2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0myh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0maccs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-40d1b990974b>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x_train, y_train, x_test, y_test, optimizer)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# run the mini-batch gradient descent to update the parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-385ffcea55ec>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, gradient_fn, x_train, y_train, x_test, y_test, params, lmbd, reg)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m               \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_relu_l1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlmbd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m               \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_relu_l1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlmbd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mreg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l2'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m               \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_relu_l2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlmbd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-ad0b6da6fc0a>\u001b[0m in \u001b[0;36mcost_relu_l1\u001b[0;34m(x, y, params, lmbd)\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0ml1reg_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlmbd\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m   \u001b[0mcost_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml1reg_cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcost_reg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-ad0b6da6fc0a>\u001b[0m in \u001b[0;36mcost_relu\u001b[0;34m(x, y, params)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mxb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mq1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mq2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}