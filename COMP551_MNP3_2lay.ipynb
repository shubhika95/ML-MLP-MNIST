{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6331ZSsQGY3"
   },
   "source": [
    "# COMP 551 - Mini-project 3\n",
    "Group 63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ifel1K0gBWt_"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "%matplotlib inline                                 \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.debugger import set_trace  \n",
    "import scipy.sparse as sparse\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b08Mmbs8lr81"
   },
   "source": [
    "## Task 1. Data pre-processing\n",
    "\n",
    "- Load the raw data from Keras.\n",
    "- Vectorize 28*28 pictures to 1D vector.\n",
    "- Normalize the intensity of the pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZyGXlaKojgz"
   },
   "source": [
    "Load the MNIST dataset distributed with Keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N01fKjwJDKAz",
    "outputId": "4c1ae7ea-cacc-4e5e-ad05-a710de52e6b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28) (60000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rX8l1wVDOiN"
   },
   "source": [
    "Vectorize the 28*28 pictures to a 784 vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y4gvmJN6DPxI",
    "outputId": "9532e485-d187-4c34-a766-ae634475bb4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.reshape(x_train, (-1, 784)).astype('float32')\n",
    "x_test = np.reshape(x_test, (-1, 784)).astype('float32')\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQqnKrdkEcw6"
   },
   "source": [
    "The intensity ranges from 0 to 255. We divide all intensities by the maximum (255) to obtain a [0-1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P-s4iFV3Ez3H",
    "outputId": "55b3ccb0-3d28-4a6f-bf31-fa2b05d58518"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intensity before normalization: 0.0 255.0\n",
      "Intensity after normalization: 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "print('Intensity before normalization:', np.amin(x_train), np.amax(x_train))\n",
    "x_train, x_test = x_train/255.0, x_test/255.0\n",
    "print('Intensity after normalization:', np.amin(x_train), np.amax(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUnquDPg1D-k"
   },
   "source": [
    "We transform the (N,) vector of labels using one-hot encoding into a (N,C) matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9fXA7QipDn4Z"
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9j0ZFP-81zIY",
    "outputId": "408de6a1-1486-4dc7-940e-ed4f5b270509"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10) (10000, 10)\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape, y_test.shape)\n",
    "print(y_train[0:3,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpKU5kZOJkEe"
   },
   "source": [
    "## Task 2. Multilayer perceptron implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6FNYQoRs0ue"
   },
   "source": [
    "### 2.1 Build the network\n",
    "Our task is a multiclass classification.The cost function will be the multi-class cross-entropy loss. We will use the following architecture:\n",
    "- output layer = softmax activation\n",
    "- 2 hidden layers: 128 units, ReLu activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUW6HFnH43cv"
   },
   "source": [
    "First, we implement the activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "L1skqPZ_s0uq"
   },
   "outputs": [],
   "source": [
    "# softmax\n",
    "eps=1e-8\n",
    "def softmax(z):\n",
    "    logits = z - np.max(z) # for numerical stability\n",
    "    sum_logits = np.sum(np.exp(logits), axis=1) +eps\n",
    "    softmax = np.exp(logits)/sum_logits[:,None] \n",
    "    return softmax\n",
    "\n",
    "# relu\n",
    "relu = lambda z: np.maximum(0,z)\n",
    "\n",
    "# derivatives of relu (formula from backprop slides)\n",
    "def relu_dv(q):\n",
    "  q[q<=0] = 0\n",
    "  q[q>0] = 1\n",
    "  return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxFKbS58_69S"
   },
   "source": [
    "Next, we build the MLP class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "oKIoaax3f6f0"
   },
   "outputs": [],
   "source": [
    "# for 2 hidden layer with relu activation\n",
    "class MLP2layer_relu:\n",
    "    \n",
    "    def __init__(self, M = 128, cost=False):\n",
    "        self.M = M\n",
    "        self.cost = cost\n",
    "            \n",
    "    def fit(self, x_train, y_train, x_test, y_test, optimizer):\n",
    "        N = x_train.shape[0]\n",
    "        C = y_train.shape[1] # number of classes\n",
    "        D = x_train.shape[1]\n",
    "        def gradient(x, y, params):\n",
    "            v, w, u = params\n",
    "            # forward pass\n",
    "            n = x.shape[0]\n",
    "            # add bias to the input layer\n",
    "            x = np.column_stack([x,np.ones(n)*0.1])\n",
    "            # add bias to the hidden layers\n",
    "            b = np.ones((n,1))*0.1\n",
    "\n",
    "            q1 = np.dot(x, v) \n",
    "            z1 = relu(np.hstack((q1,b))) \n",
    "            q2 = np.dot(z1,w) \n",
    "            z2 = relu(np.hstack((q2,b)))\n",
    "            yh = softmax(np.dot(z2, u))#N x C\n",
    "            # backward pass \n",
    "            dy = yh - y #N x C\n",
    "            du = np.dot(z2.T,dy)/N \n",
    "            dz2 = np.dot(dy,u.T)\n",
    "            dz2 = np.delete(dz2, -1, axis=1)\n",
    "            dq2 = relu_dv(q2)\n",
    "            dw = np.dot(z1.T, dz2 * dq2)/N #M x C\n",
    "            dz1 = np.dot(dz2, w.T) #N x M\n",
    "            dz1 = np.delete(dz1,-1,axis=1)\n",
    "            dq1 = relu_dv(q1)\n",
    "            dv = np.dot(x.T, dz1 * dq1)/N #D x M\n",
    "            dparams = [dv, dw, du]\n",
    "            return dparams\n",
    "        \n",
    "        # initialize the parameters with values in the standard normal distribution and scaled to be low\n",
    "        u = np.random.randn(self.M+1,C) * 0.1 #M x C\n",
    "        w = np.random.randn(self.M+1,self.M) * .01 #M x M\n",
    "        v = np.random.randn(D+1,self.M) * .01 #D x M\n",
    "        \n",
    "        params0 = [v,w,u]\n",
    "\n",
    "        # run the mini-batch gradient descent to update the parameters\n",
    "        if self.cost == True:\n",
    "            self.params, self.train_loss, self.test_loss = optimizer.run(gradient, x_train, y_train, x_test, y_test, params0)\n",
    "        else:\n",
    "            self.params = optimizer.run(gradient, x_train, y_train, x_test, y_test, params0)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        v, w, u = self.params\n",
    "        # add bias to the input layer\n",
    "        Nt = x.shape[0]\n",
    "        x = np.column_stack([x,np.ones(Nt)*0.1])\n",
    "        b1 = np.ones((Nt,1))*0.1\n",
    "     \n",
    "        # forward pass only using updated parameters\n",
    "\n",
    "        q1 = np.dot(x,v)\n",
    "        z1 = relu(np.hstack((q1,b1)))\n",
    "        q2 = np.dot(z1,w)\n",
    "        z2 = relu(np.hstack((q2,b1)))\n",
    "        yh = softmax(np.dot(z2, u))#N x C\n",
    "        return yh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65sh_GyGNkqO"
   },
   "source": [
    "### 2.2 Implement the cost and accuracy function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IgiPvgSbNuZG"
   },
   "outputs": [],
   "source": [
    "# Softmax cross entropy \n",
    "def logsumexp(Z):                                                # dimension N x C\n",
    "    Zmax = np.max(Z,axis=1)[:,None]                              # max over C\n",
    "    log_sum_exp = Zmax + np.log(np.sum(np.exp(Z - Zmax), axis=1))\n",
    "    return log_sum_exp\n",
    "\n",
    "# cost for relu activation - two layers\n",
    "def cost_relu(x, y, params):\n",
    "  Nt = x.shape[0]\n",
    "  v, w, u = params\n",
    "  b1 = np.ones((Nt,1))*0.1\n",
    "  xb = np.column_stack([x,np.ones(Nt)*0.1])\n",
    "  q1 = np.dot(xb, v) \n",
    "  z1 = relu(np.hstack((q1,b1))) \n",
    "  q2 = np.dot(z1,w)\n",
    "  z2 = relu(np.hstack((q2,b1)))\n",
    "  q3 = np.dot(z2, u) #N x C\n",
    "  nll = - np.mean(np.sum(q3*y, 1) - logsumexp(q3)) \n",
    "  return nll\n",
    "  \n",
    "\n",
    "# Accuracy\n",
    "def evaluate_acc(y, yh):\n",
    "  y_pred = np.argmax(yh,axis=1)\n",
    "  accuracy = np.count_nonzero(y_pred == np.argmax(y,axis=1))/y.shape[0]\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhwM1bbNO1Fe"
   },
   "source": [
    "### 2.3. Implement the cross-validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MDnvWrREO42b"
   },
   "outputs": [],
   "source": [
    "def cross_validate(n, n_folds=6):\n",
    "    #get the number of data samples in each split\n",
    "    n_val = n // n_folds\n",
    "    inds = np.random.permutation(n)\n",
    "    inds = []\n",
    "    for f in range(n_folds):\n",
    "        tr_inds = []\n",
    "        #get the validation indexes\n",
    "        val_inds = list(range(f * n_val, (f+1)*n_val))\n",
    "        #get the train indexes\n",
    "        if f > 0:\n",
    "            tr_inds = list(range(f*n_val))\n",
    "        if f < n_folds - 1:\n",
    "            tr_inds = tr_inds + list(range((f+1)*n_val, n))\n",
    "        #The yield statement suspends functionâ€™s execution and sends a value back to the caller\n",
    "        #but retains enough state information to enable function to resume where it is left off\n",
    "        yield tr_inds, val_inds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzCYnVR2OZ7c"
   },
   "source": [
    "### 2.4 Implement the optimizer\n",
    "\n",
    "We will use a mini-batch gradient-descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1u1OikUgk3hk"
   },
   "outputs": [],
   "source": [
    "def create_mini_batch(x, y, batch_size): \n",
    "    D = x.shape[1]\n",
    "    data = np.hstack((x, y))\n",
    "    np.random.shuffle(data)\n",
    "    mini = data[:batch_size,:]                                                    \n",
    "    x_mini = mini[:,:D]\n",
    "    y_mini = mini[:,D:]\n",
    "    return x_mini, y_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AhCrG89Es0us"
   },
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "    \n",
    "    def __init__(self, learning_rate=.001, epsilon=1e-8, batch_size=100, iters=600, epochs=50, cost=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iters = iters\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.cost = cost\n",
    "        \n",
    "    def run(self, gradient_fn, x_train, y_train, x_test, y_test, params):\n",
    "      if self.cost == True:\n",
    "        epoch = 1\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        for epoch in range(self.epochs):\n",
    "          train_epoch_loss = []\n",
    "          test_epoch_loss = []\n",
    "          for t in range(self.iters):\n",
    "            x_mini, y_mini = create_mini_batch(x_train, y_train, self.batch_size)\n",
    "            train_loss = cost_relu(x_mini, y_mini, params)\n",
    "            test_loss = cost_relu(x_test, y_test, params)\n",
    "            grad = gradient_fn(x_mini, y_mini, params)\n",
    "            for p in range(len(params)):\n",
    "                params[p] -= self.learning_rate * grad[p]\n",
    "            if t % self.iters == 0:\n",
    "              print(f\"Epoch: {epoch+1}, Train error: {train_loss:.4f}, Test error: {test_loss:.4f}\")\n",
    "              epoch += 1\n",
    "            train_epoch_loss.append(train_loss)\n",
    "            test_epoch_loss.append(test_loss)\n",
    "          train_losses.append(np.mean(train_epoch_loss))\n",
    "          test_losses.append(np.mean(test_epoch_loss))\n",
    "        return params, train_losses, test_losses\n",
    "      else:     \n",
    "        norms = np.array([np.inf])\n",
    "        t = 1\n",
    "        while np.any(norms > self.epsilon) and t<self.iters:\n",
    "            # create mini-batch\n",
    "            x_mini, y_mini = create_mini_batch(x_train, y_train, self.batch_size)\n",
    "            # calculate gradient for the mini-batch\n",
    "            grad = gradient_fn(x_mini, y_mini, params)\n",
    "            # update v and dw\n",
    "            for p in range(len(params)):\n",
    "                params[p] -= self.learning_rate * grad[p]\n",
    "            t += 1\n",
    "            norms = np.array([np.linalg.norm(g) for g in grad])\n",
    "        return params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlT4tqWfOqH1"
   },
   "source": [
    "## Task 3. Run the experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJ-WzYlJ39Un"
   },
   "source": [
    "### 3.1. Learning rate tuning with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "FTtvzLriPGhi",
    "outputId": "17562f4f-c211-4dc2-d961-e99baf4278c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For learning rate = 0.1 and fold 0 the accuracy is: 0.56585\n",
      "For learning rate = 0.1 and fold 1 the accuracy is: 0.57295\n",
      "For learning rate = 0.1 and fold 2 the accuracy is: 0.59255\n",
      "For learning rate = 1 and fold 0 the accuracy is: 0.91285\n",
      "For learning rate = 1 and fold 1 the accuracy is: 0.90925\n",
      "For learning rate = 1 and fold 2 the accuracy is: 0.9145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in subtract\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For learning rate = 10 and fold 0 the accuracy is: 0.0997\n",
      "For learning rate = 10 and fold 1 the accuracy is: 0.0965\n",
      "For learning rate = 10 and fold 2 the accuracy is: 0.09995\n"
     ]
    }
   ],
   "source": [
    "learning_list = [0.1,1,10]\n",
    "\n",
    "num_folds = 3\n",
    "acc_valid = np.zeros((len(learning_list), num_folds))\n",
    "for i, learning in enumerate(learning_list):\n",
    "    #Find the validation accuracy for num_folds splits for a given learning rate\n",
    "    for f, (tr, val) in enumerate(cross_validate(x_train.shape[0], num_folds)):\n",
    "        model = MLP2layer_relu(cost=False)\n",
    "        optimizer = GradientDescent(learning_rate=learning, epochs=100, iters=20000, cost=False)\n",
    "        model.fit(x_train[tr], y_train[tr], x_test, y_test, optimizer)\n",
    "        acc_valid[i, f] = evaluate_acc(y_train[val], model.predict(x_train[val]))\n",
    "        print('For learning rate =',learning,'and fold',f,'the accuracy is:',acc_valid[i, f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.errorbar(learning_list, np.mean(acc_valid, axis=1), np.std(acc_valid, axis=1))\n",
    "plt.xlabel('Learning rate')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For learning rate = 2 and fold 0 the accuracy is: 0.9356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in subtract\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For learning rate = 2 and fold 1 the accuracy is: 0.0965\n",
      "For learning rate = 2 and fold 2 the accuracy is: 0.93715\n",
      "For learning rate = 5 and fold 0 the accuracy is: 0.0997\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-27d1f493178a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP2layer_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0macc_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'For learning rate ='\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'and fold'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'the accuracy is:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-d40bc0a17864>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x_train, y_train, x_test, y_test, optimizer)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-98f6b3671b36>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, gradient_fn, x_train, y_train, x_test, y_test, params)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorms\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# create mini-batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mx_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0;31m# calculate gradient for the mini-batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-1b119249170a>\u001b[0m in \u001b[0;36mcreate_mini_batch\u001b[0;34m(x, y, batch_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_list = [2,5,7]\n",
    "\n",
    "num_folds = 3\n",
    "acc_valid = np.zeros((len(learning_list), num_folds))\n",
    "for i, learning in enumerate(learning_list):\n",
    "    #Find the validation accuracy for num_folds splits for a given learning rate\n",
    "    for f, (tr, val) in enumerate(cross_validate(x_train.shape[0], num_folds)):\n",
    "        model = MLP2layer_relu(cost=False)\n",
    "        optimizer = GradientDescent(learning_rate=learning, epochs=100, iters=20000, cost=False)\n",
    "        model.fit(x_train[tr], y_train[tr], x_test, y_test, optimizer)\n",
    "        acc_valid[i, f] = evaluate_acc(y_train[val], model.predict(x_train[val]))\n",
    "        print('For learning rate =',learning,'and fold',f,'the accuracy is:',acc_valid[i, f])\n",
    "# Due to the very long running time we stopped the algorithm when we saw that 5 was not giving a good accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in subtract\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For learning rate = 2.2 and fold 0 the accuracy is: 0.0997\n",
      "For learning rate = 2.2 and fold 1 the accuracy is: 0.935\n",
      "For learning rate = 2.2 and fold 2 the accuracy is: 0.94075\n",
      "For learning rate = 2.5 and fold 0 the accuracy is: 0.94585\n",
      "For learning rate = 2.5 and fold 1 the accuracy is: 0.94185\n",
      "For learning rate = 2.5 and fold 2 the accuracy is: 0.9452\n",
      "For learning rate = 2.8 and fold 0 the accuracy is: 0.0997\n",
      "For learning rate = 2.8 and fold 1 the accuracy is: 0.94405\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-8469ddfcb3ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP2layer_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0macc_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'For learning rate ='\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'and fold'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'the accuracy is:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-d40bc0a17864>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x_train, y_train, x_test, y_test, optimizer)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-98f6b3671b36>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, gradient_fn, x_train, y_train, x_test, y_test, params)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorms\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# create mini-batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mx_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0;31m# calculate gradient for the mini-batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-1b119249170a>\u001b[0m in \u001b[0;36mcreate_mini_batch\u001b[0;34m(x, y, batch_size)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mx_mini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmini\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_list = [2.2,2.5,2.8]\n",
    "\n",
    "num_folds = 3\n",
    "acc_valid = np.zeros((len(learning_list), num_folds))\n",
    "for i, learning in enumerate(learning_list):\n",
    "    #Find the validation accuracy for num_folds splits for a given learning rate\n",
    "    for f, (tr, val) in enumerate(cross_validate(x_train.shape[0], num_folds)):\n",
    "        model = MLP2layer_relu(cost=False)\n",
    "        optimizer = GradientDescent(learning_rate=learning, epochs=100, iters=20000, cost=False)\n",
    "        model.fit(x_train[tr], y_train[tr], x_test, y_test, optimizer)\n",
    "        acc_valid[i, f] = evaluate_acc(y_train[val], model.predict(x_train[val]))\n",
    "        print('For learning rate =',learning,'and fold',f,'the accuracy is:',acc_valid[i, f])\n",
    "# Due to the very long running time we stopped the algorithm when we saw that 2.5 was giving a good accuracy and 2.8 had numercial problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Performance of the model and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy is 93.7.\n"
     ]
    }
   ],
   "source": [
    "model = MLP2layer_relu()\n",
    "optimizer = GradientDescent(learning_rate=2.5, epochs=100, iters=20000, cost=False)\n",
    "model.fit(x_train, y_train, x_test, y_test, optimizer)\n",
    "acc = evaluate_acc(y_test, model.predict(x_test))\n",
    "print(f'the accuracy is {acc*100:.1f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 959.    0.    1.    1.    0.    3.   10.    2.    4.    0.]\n",
      " [   0. 1111.    4.    2.    1.    1.    3.    2.   11.    0.]\n",
      " [   8.    7.  953.   12.    7.    2.   11.   10.   19.    3.]\n",
      " [   0.    0.   17.  943.    1.   17.    2.   14.   13.    3.]\n",
      " [   1.    1.    5.    1.  931.    0.   12.    3.    5.   23.]\n",
      " [   8.    1.    3.   32.    8.  793.   14.    4.   22.    7.]\n",
      " [   9.    3.    4.    1.   12.   11.  915.    0.    3.    0.]\n",
      " [   3.    7.   24.    5.    6.    0.    0.  964.    3.   16.]\n",
      " [   7.    5.    5.   20.   10.   22.   10.   13.  879.    3.]\n",
      " [   9.    6.    1.   11.   38.    8.    1.   11.    6.  918.]]\n"
     ]
    }
   ],
   "source": [
    "yh = model.predict(x_test)\n",
    "def confusion_matrix(y, yh):\n",
    "    n_classes = y.shape[1]\n",
    "    c_matrix = np.zeros((n_classes, n_classes))\n",
    "    for c1 in range(n_classes):\n",
    "        for c2 in range(n_classes):\n",
    "        #(y==c1)*(yh==c2) is 1 when both conditions are true or 0\n",
    "            c_matrix[c1, c2] = np.sum((np.argmax(y,axis=1)==c1)*(np.argmax(yh,axis=1)==c2))\n",
    "    return c_matrix\n",
    "\n",
    "cmat = confusion_matrix(y_test, yh)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(cmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Train and test cost as function of training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train error: 2.3020, Test error: 2.3021\n",
      "Epoch: 2, Train error: 2.2569, Test error: 2.2577\n",
      "Epoch: 3, Train error: 1.9790, Test error: 1.9878\n",
      "Epoch: 4, Train error: 1.3628, Test error: 1.2799\n",
      "Epoch: 5, Train error: 0.8210, Test error: 0.7918\n",
      "Epoch: 6, Train error: 0.6092, Test error: 0.6055\n",
      "Epoch: 7, Train error: 0.4820, Test error: 0.5104\n",
      "Epoch: 8, Train error: 0.4677, Test error: 0.4533\n",
      "Epoch: 9, Train error: 0.3923, Test error: 0.4184\n",
      "Epoch: 10, Train error: 0.2578, Test error: 0.3923\n",
      "Epoch: 11, Train error: 0.3072, Test error: 0.3748\n",
      "Epoch: 12, Train error: 0.3654, Test error: 0.3598\n",
      "Epoch: 13, Train error: 0.3130, Test error: 0.3488\n",
      "Epoch: 14, Train error: 0.3784, Test error: 0.3376\n",
      "Epoch: 15, Train error: 0.2986, Test error: 0.3295\n",
      "Epoch: 16, Train error: 0.3668, Test error: 0.3205\n",
      "Epoch: 17, Train error: 0.4108, Test error: 0.3127\n",
      "Epoch: 18, Train error: 0.3395, Test error: 0.3066\n",
      "Epoch: 19, Train error: 0.2406, Test error: 0.3000\n",
      "Epoch: 20, Train error: 0.3678, Test error: 0.2955\n",
      "Epoch: 21, Train error: 0.2354, Test error: 0.2886\n",
      "Epoch: 22, Train error: 0.5712, Test error: 0.2839\n",
      "Epoch: 23, Train error: 0.3965, Test error: 0.2789\n",
      "Epoch: 24, Train error: 0.3219, Test error: 0.2732\n",
      "Epoch: 25, Train error: 0.2780, Test error: 0.2692\n",
      "Epoch: 26, Train error: 0.4630, Test error: 0.2641\n",
      "Epoch: 27, Train error: 0.3345, Test error: 0.2600\n",
      "Epoch: 28, Train error: 0.2580, Test error: 0.2565\n",
      "Epoch: 29, Train error: 0.2440, Test error: 0.2530\n",
      "Epoch: 30, Train error: 0.2498, Test error: 0.2495\n",
      "Epoch: 31, Train error: 0.2570, Test error: 0.2455\n",
      "Epoch: 32, Train error: 0.1340, Test error: 0.2407\n",
      "Epoch: 33, Train error: 0.1140, Test error: 0.2363\n",
      "Epoch: 34, Train error: 0.2869, Test error: 0.2331\n",
      "Epoch: 35, Train error: 0.2376, Test error: 0.2295\n",
      "Epoch: 36, Train error: 0.1989, Test error: 0.2256\n",
      "Epoch: 37, Train error: 0.2738, Test error: 0.2226\n",
      "Epoch: 38, Train error: 0.1787, Test error: 0.2197\n",
      "Epoch: 39, Train error: 0.2738, Test error: 0.2154\n",
      "Epoch: 40, Train error: 0.1315, Test error: 0.2118\n",
      "Epoch: 41, Train error: 0.1691, Test error: 0.2094\n",
      "Epoch: 42, Train error: 0.2450, Test error: 0.2062\n",
      "Epoch: 43, Train error: 0.2790, Test error: 0.2030\n",
      "Epoch: 44, Train error: 0.1620, Test error: 0.2001\n",
      "Epoch: 45, Train error: 0.0963, Test error: 0.1966\n",
      "Epoch: 46, Train error: 0.1697, Test error: 0.1948\n",
      "Epoch: 47, Train error: 0.1185, Test error: 0.1920\n",
      "Epoch: 48, Train error: 0.1149, Test error: 0.1890\n",
      "Epoch: 49, Train error: 0.1464, Test error: 0.1864\n",
      "Epoch: 50, Train error: 0.1397, Test error: 0.1834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MLP2layer_relu at 0x7f18c84cbc10>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP2layer_relu(cost=True)\n",
    "optimizer = GradientDescent(learning_rate=2.5, epochs=50, iters=600, cost=True)\n",
    "\n",
    "model.fit(x_train, y_train, x_test, y_test, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(model.train_loss)), model.train_loss, '-', label='train')\n",
    "plt.plot(np.arange(len(model.test_loss)), model.test_loss, '-', label='test')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.ylim(top = 0.5)\n",
    "plt.savefig('epochs_2layers.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "COMP551_MNP3_2lay.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
