{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "standard-passion",
   "metadata": {},
   "source": [
    "# COMP 551 - Mini-project 3\n",
    "Group 63"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-wonder",
   "metadata": {},
   "source": [
    "### Load the data and vectorize it to be loaded into MLP architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "instant-award",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# use the unormalized data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "num_train, num_test = x_train.shape[0], x_test.shape[0]\n",
    "x_train = x_train.reshape(num_train, -1)    # vectorization\n",
    "x_test = x_test.reshape(num_test, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-bernard",
   "metadata": {},
   "source": [
    "### Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "least-motorcycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 regularisation\n",
    "def compute_cost_with_regularization(A3, Y, parameters, lambd):\n",
    "    m = Y.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    \n",
    "    cross_entropy_cost = cross_entropy_loss(A3, Y) # This gives you the cross-entropy part of the cost\n",
    "\n",
    "    L2_regularization_cost = lambd * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3))) / (2 * m)\n",
    "\n",
    "    cost = cross_entropy_cost + L2_regularization_cost\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "continued-devices",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define all the activation function\n",
    "\n",
    "def relu():\n",
    "    return Relu()\n",
    "\n",
    "\n",
    "def sigmoid():\n",
    "    return Sigmoid()\n",
    "\n",
    "\n",
    "def tanh():\n",
    "    return Tanh()\n",
    "\n",
    "\n",
    "def softmax(x):     # softmax\n",
    "    nx = x - np.max(x, axis=1, keepdims=True)\n",
    "    exp_x = np.exp(nx)\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):     # cross entropy as the loss function\n",
    "    batch = y_true.shape[0]\n",
    "    return -np.sum(np.log(y_pred[:, y_true] + 1e-7)) / batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "unexpected-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:     # relu activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.mask = x <= 0\n",
    "\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, d): #take derevative for the backward process\n",
    "        d[self.mask] = 0\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cooked-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:       # model x*w + b & error backprop\n",
    "\n",
    "    def __init__(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.y = np.dot(x, self.w) + self.b\n",
    "        return self.y\n",
    "\n",
    "    def backward(self, d):\n",
    "        dx = np.dot(d, self.w.T)\n",
    "        self.dw = np.dot(self.x.T, d)\n",
    "        self.db = np.sum(d, axis=0)\n",
    "        return dx\n",
    "\n",
    "\n",
    "class SoftmaxLoss:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, y_true):\n",
    "        self.y_true = y_true\n",
    "        self.y_pred = softmax(x)\n",
    "        self.loss = cross_entropy_loss(self.y_pred, self.y_true)\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self):\n",
    "        class_num = self.y_pred.shape[1]\n",
    "        batch = self.y_true.shape[0]\n",
    "        y_true_one_hot = np.zeros((batch, class_num))\n",
    "        y_true_one_hot[np.arange(batch), self.y_true] = 1\n",
    "        return (self.y_pred - y_true_one_hot) / batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "italic-cornwall",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:      # sigmoid activation function\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.y = 1.0/(1 + np.exp(-x))\n",
    "        return self.y\n",
    "\n",
    "    def backward(self, d): #take derevative for the backward process\n",
    "        return d * (1.0 - self.y) * self.y\n",
    "\n",
    "\n",
    "class Tanh:     # tanh activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def backward(self, d): #take derevative for the backward process\n",
    "        return d * (1-np.tanh(self.x) ** 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-england",
   "metadata": {},
   "source": [
    "### Build the MLP architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "spiritual-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the multilayer perceptron class to fit and predit the images\n",
    "\n",
    "class MLP:\n",
    "\n",
    "    def __init__(self, input_layer=784, hidden_layer= [], output_layer=10, max_episode=50000,\n",
    "                 active_func=relu, batch_size=200, learing_rate=0.1): # intotal 784 pixel inputs, 10 outputs refers to integer 0-9\n",
    "        size = input_layer\n",
    "        self.layers = []\n",
    "        self.cal_layer = []\n",
    "\n",
    "        for hidden in hidden_layer:\n",
    "            w, b = np.random.randn(size, hidden), np.random.randn(1, hidden)        # w, b refer to \"weight\" and \"bias\"\n",
    "            self.layers.append({'w': w, 'b': b})\n",
    "            self.cal_layer.append(Affine(w, b))\n",
    "            self.cal_layer.append(active_func())\n",
    "            size = hidden\n",
    "\n",
    "        w, b = np.random.randn(size, output_layer), np.random.randn(1, output_layer)\n",
    "        self.cal_layer.append(Affine(w, b))\n",
    "        self.layers.append({'w': w, 'b': b})\n",
    "        self.active_func = active_func\n",
    "        # self.de_active_func = active_func[1]\n",
    "        self.max_episode = max_episode\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = learing_rate\n",
    "\n",
    "        self.out_layer = SoftmaxLoss()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for ep in range(self.max_episode):      # number of gradient descent\n",
    "            # random select some data\n",
    "            batch = np.random.choice(X.shape[0], self.batch_size)       # sample a part of data each time for implementing mini-batch\n",
    "            x, y_true = X[batch], y[batch]\n",
    "\n",
    "            # forward               \n",
    "            for cal in self.cal_layer:\n",
    "                x = cal.forward(x)\n",
    "            self.out_layer.forward(x, y_true)\n",
    "\n",
    "            # backward              # backprop, calculate gradient each iteration\n",
    "            d = self.out_layer.backward()\n",
    "            for cal in reversed(self.cal_layer):\n",
    "                d = cal.backward(d)\n",
    "\n",
    "            # check grad\n",
    "            grad = []\n",
    "            for cal in self.cal_layer:\n",
    "                if isinstance(cal, Affine):     # only Affine has parameters needed to be updated\n",
    "                    grad.append({'w': cal.dw, 'b': cal.db})\n",
    "\n",
    "            # update grad\n",
    "            for i in range(len(self.layers)):       # update parameter at each iteration -> SGD\n",
    "                self.layers[i]['w'] -= self.lr * grad[i]['w']\n",
    "                self.layers[i]['b'] -= self.lr * grad[i]['b']\n",
    "\n",
    "            \n",
    "\n",
    "    def predict(self, X):\n",
    "        x = X\n",
    "        # for i in range(len(self.layers) - 1):\n",
    "        #     w, b = self.layers[i]['w'], self.layers[i]['b']\n",
    "        #     x = self.active_func(np.dot(x, w) + b)\n",
    "\n",
    "        # w, b = self.layers[len(self.layers) - 1]['w'], self.layers[len(self.layers) - 1]['b']\n",
    "        # x = np.dot(x, w) + b\n",
    "        # y = np.argmax(x, axis=1)\n",
    "        # return y\n",
    "\n",
    "        for cal in self.cal_layer:\n",
    "            x = cal.forward(x)\n",
    "        # self.out_layer.forward(x, y_true)\n",
    "\n",
    "        x = softmax(x)\n",
    "        y = np.argmax(x, axis=1)\n",
    "        return y\n",
    "\n",
    "\n",
    "    def evaluate(self, y_true, y_predict):\n",
    "        return sum(y_true == y_predict) / len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-thinking",
   "metadata": {},
   "source": [
    "### Parameter defining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "confused-annual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc  0.9009  with parameter:  {'hidden_layer': [], 'max_episode': 10000, 'batch_size': 100, 'learing_rate': 0.01}\n",
      "Acc  0.8744  with parameter:  {'hidden_layer': [], 'max_episode': 10000, 'batch_size': 100, 'learing_rate': 0.1}\n",
      "Acc  0.8976  with parameter:  {'hidden_layer': [], 'max_episode': 10000, 'batch_size': 100, 'learing_rate': 1}\n",
      "Acc  0.8304  with parameter:  {'hidden_layer': [], 'max_episode': 10000, 'batch_size': 100, 'learing_rate': 2}\n",
      "Acc  0.8926  with parameter:  {'hidden_layer': [], 'max_episode': 10000, 'batch_size': 100, 'learing_rate': 5}\n",
      "Best acc:  0.9009\n",
      "Best mlp parameters:  {'hidden_layer': [], 'max_episode': 10000, 'batch_size': 100, 'learing_rate': 0.01}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "parameters = {\n",
    "    #\"hidden_layer\": [[], [128], [128, 128]],\n",
    "    \"hidden_layer\": [[]],\n",
    "    \"max_episode\": [10000],\n",
    "    \"batch_size\": [100],\n",
    "    \"learing_rate\": [0.01, 0.1, 1, 2, 5]\n",
    "}\n",
    "\n",
    "# Grid Search\n",
    "\n",
    "from  copy import deepcopy\n",
    "def generate_all():\n",
    "    keys = list(parameters.keys())\n",
    "    parameters_list = [{}]\n",
    "\n",
    "    for key in keys:\n",
    "        tmp = []\n",
    "        for val in parameters[key]:\n",
    "            for pre in parameters_list:\n",
    "                pre = deepcopy(pre)\n",
    "                pre[key] = val\n",
    "                tmp.append(pre)\n",
    "        parameters_list = tmp\n",
    "    return parameters_list\n",
    "\n",
    "best, best_p, best_acc = None, None, 0\n",
    "\n",
    "for p in generate_all():\n",
    "    mlp = MLP(**p)\n",
    "    mlp.fit(x_train, y_train)\n",
    "    acc = mlp.evaluate(y_test, mlp.predict(x_test))\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_p = p\n",
    "        best = mlp\n",
    "    print(\"Acc \", acc, \" with parameter: \", p)\n",
    "\n",
    "print(\"Best acc: \", best_acc)\n",
    "print(\"Best mlp parameters: \", best_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "unsigned-addition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc  0.8914  with parameter:  {'hidden_layer': [], 'max_episode': 10000, 'batch_size': 100, 'learing_rate': 0.01}\n",
      "Acc  0.115  with parameter:  {'hidden_layer': [128], 'max_episode': 10000, 'batch_size': 100, 'learing_rate': 0.01}\n",
      "Acc  0.1135  with parameter:  {'hidden_layer': [128, 128], 'max_episode': 10000, 'batch_size': 100, 'learing_rate': 0.01}\n",
      "Best acc:  0.8914\n",
      "Best mlp parameters:  {'hidden_layer': [], 'max_episode': 10000, 'batch_size': 100, 'learing_rate': 0.01}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "parameters = {\n",
    "    \"hidden_layer\": [[], [128], [128, 128]],\n",
    "    #\"hidden_layer\": [[]],\n",
    "    \"max_episode\": [10000],\n",
    "    \"batch_size\": [100],\n",
    "    \"learing_rate\": [0.01]\n",
    "    #\"learing_rate\": [0.1, 1, 2, 5]\n",
    "}\n",
    "\n",
    "# Grid Search\n",
    "\n",
    "from  copy import deepcopy\n",
    "def generate_all():\n",
    "    keys = list(parameters.keys())\n",
    "    parameters_list = [{}]\n",
    "\n",
    "    for key in keys:\n",
    "        tmp = []\n",
    "        for val in parameters[key]:\n",
    "            for pre in parameters_list:\n",
    "                pre = deepcopy(pre)\n",
    "                pre[key] = val\n",
    "                tmp.append(pre)\n",
    "        parameters_list = tmp\n",
    "    return parameters_list\n",
    "\n",
    "best, best_p, best_acc = None, None, 0\n",
    "\n",
    "for p in generate_all():\n",
    "    mlp = MLP(**p)\n",
    "    mlp.fit(x_train, y_train)\n",
    "    acc = mlp.evaluate(y_test, mlp.predict(x_test))\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_p = p\n",
    "        best = mlp\n",
    "    print(\"Acc \", acc, \" with parameter: \", p)\n",
    "\n",
    "print(\"Best acc: \", best_acc)\n",
    "print(\"Best mlp parameters: \", best_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-puppy",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"2 -> Acc  0.6848  with parameter:  {'hidden_layer': [], 'max_episode': 10000, 'batch_size': 100, 'learing_rate': 2}\n",
    "Acc  0.0982  with parameter:  {'hidden_layer': [128], 'max_episode': 10000, 'batch_size': 100, 'learing_rate': 2}\n",
    "Acc  0.0983  with parameter:  {'hidden_layer': [128, 128], 'max_episode': 10000, 'batch_size': 100, 'learing_rate': 2}\n",
    "Best acc:  0.6848\n",
    "Best mlp parameters:  {'hidden_layer': [], 'max_episode': 10000, 'batch_size': 100, 'learing_rate': 2}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-neighborhood",
   "metadata": {},
   "source": [
    "### Test the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "handy-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(mlp):\n",
    "\n",
    "    mlp.fit(x_train, y_train)\n",
    "    yp = mlp.predict(x_test)\n",
    "    print(\"after train, on test set:\", mlp.evaluate(y_test, yp))\n",
    "    print(\"after train, on train set:\", mlp.evaluate(y_train, mlp.predict(x_train)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-technician",
   "metadata": {},
   "source": [
    "No hidden layer with 0.01 gives good accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "listed-president",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after train, on test set: 0.9144\n",
      "after train, on train set: 0.9239666666666667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(MLP(hidden_layer=[], max_episode=20000, learing_rate=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-singing",
   "metadata": {},
   "source": [
    "One hidden layer with ReLu function with 0.01 does not perform very well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "skilled-brown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after train, on test set: 0.1632\n",
      "after train, on train set: 0.17436666666666667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(MLP(hidden_layer=[128], max_episode=20000, learing_rate=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-collective",
   "metadata": {},
   "source": [
    "Two hidden layer with ReLu function with different learning rates also does not perform very well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "skilled-albany",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after train, on test set: 0.1135\n",
      "after train, on train set: 0.11236666666666667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(MLP(hidden_layer=[128, 128], max_episode=20000, learing_rate=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acknowledged-darkness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after train, on test set: 0.1135\n",
      "after train, on train set: 0.11236666666666667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(MLP(hidden_layer=[128, 64], max_episode=20000, learing_rate=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "biblical-buying",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after train, on test set: 0.1137\n",
      "after train, on train set: 0.11245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(MLP(hidden_layer=[128, 128], max_episode=100000, learing_rate=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "domestic-occasion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after train, on test set: 0.2185\n",
      "after train, on train set: 0.22263333333333332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(MLP(hidden_layer=[128, 128], max_episode=100000, learing_rate=0.0001))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-pennsylvania",
   "metadata": {},
   "source": [
    "#### Lowering the learning rate considerably gives good accuracy for ReLu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "european-tennis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after train, on test set: 0.9039\n",
      "after train, on train set: 0.9131333333333334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(MLP(hidden_layer=[128, 128], max_episode=100000, learing_rate=0.000001))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-chambers",
   "metadata": {},
   "source": [
    "#### 0.1 learning rate gives good accuracy for 2 hidden layer sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "vital-insurance",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after train, on test set: 0.9425\n",
      "after train, on train set: 0.9558833333333333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(MLP(hidden_layer=[128, 128], max_episode=100000, learing_rate=0.1, active_func = sigmoid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-research",
   "metadata": {},
   "source": [
    "#### 0.1 learning rate gives good accuracy for 2 hidden layer tanh activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "solved-trash",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after train, on test set: 0.8913\n",
      "after train, on train set: 0.89425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(MLP(hidden_layer=[128, 128], max_episode=100000, learing_rate=0.1, active_func = tanh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-expansion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
